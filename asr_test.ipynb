{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "machine_shape": "hm",
            "gpuType": "L4",
            "include_colab_link": true
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "view-in-github"
            },
            "source": [
                "# Attack Success Rate (ASR) Evaluation\n",
                "\n",
                "This notebook evaluates the performance of a pre-trained ViT Generator on the **Test Set** of the Oxford Pets dataset.\n",
                "\n",
                "**Goals:**\n",
                "1.  Load the pre-trained (untargeted) generator.\n",
                "2.  Load the Surrogate Model (RN50).\n",
                "3.  Evaluate Clean Accuracy vs. Adversarial Accuracy on unseen Test Data.\n",
                "4.  Calculate ASR (Attack Success Rate)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "id": "mount_drive"
            },
            "outputs": [],
            "source": [
                "# 1) Mount Google Drive\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive', force_remount=True)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "id": "setup_repo"
            },
            "outputs": [],
            "source": [
                "# 2) Setup Repo & Dependencies\n",
                "!nvidia-smi\n",
                "%cd /content\n",
                "import os\n",
                "if not os.path.exists(\"MFCLIP_acv\"):\n",
                "    !git clone -b hamza/discrim https://github.com/1hamzaiqbal/MFCLIP_acv\n",
                "%cd MFCLIP_acv\n",
                "!git fetch --all\n",
                "!git reset --hard origin/hamza/discrim # Force sync to latest commit\n",
                "\n",
                "!pip install torch torchvision timm einops yacs tqdm opencv-python scikit-learn scipy pyyaml ruamel.yaml pytorch-ignite foolbox pandas matplotlib seaborn wilds ftfy\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "id": "setup_data"
            },
            "outputs": [],
            "source": [
                "# 3) Setup Data (Oxford Pets)\n",
                "import shutil\n",
                "import os\n",
                "from torchvision.datasets import OxfordIIITPet\n",
                "from torchvision import transforms\n",
                "from pathlib import Path\n",
                "\n",
                "# Download Dataset\n",
                "root = Path(\"/content/data/oxford_pets\")\n",
                "root.mkdir(parents=True, exist_ok=True)\n",
                "_ = OxfordIIITPet(root=str(root), download=True, transform=transforms.ToTensor())\n",
                "\n",
                "# Fetch Annotations\n",
                "%cd /content\n",
                "!mkdir -p /content/data/oxford_pets\n",
                "!wget -q https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n",
                "!wget -q https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\n",
                "!tar -xf images.tar.gz -C /content/data/oxford_pets\n",
                "!tar -xf annotations.tar.gz -C /content/data/oxford_pets\n",
                "\n",
                "# Copy Surrogate Checkpoint (Required for Evaluation)\n",
                "src_ckpt = \"/content/drive/MyDrive/grad/comp_vision/hanson_loss/oxford_pets/RN50_ArcFace_oxford_pets.pth\"\n",
                "dst_ckpt = \"/content/data/oxford_pets/RN50_ArcFace.pth\"\n",
                "\n",
                "if os.path.exists(src_ckpt):\n",
                "    shutil.copy(src_ckpt, dst_ckpt)\n",
                "    print(f\"Successfully copied surrogate checkpoint to {dst_ckpt}\")\n",
                "else:\n",
                "    raise FileNotFoundError(f\"Surrogate checkpoint not found at {src_ckpt}. Cannot evaluate without the victim model!\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "id": "load_models"
            },
            "outputs": [],
            "source": [
                "# 4) Load Models & Evaluate\n",
                "%cd /content/MFCLIP_acv\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "from model import ViTGenerator\n",
                "from utils.util import setup_cfg\n",
                "from dass.engine import build_trainer\n",
                "from loss.head.head_def import HeadFactory\n",
                "from torchvision import transforms\n",
                "from ruamel.yaml import YAML\n",
                "import argparse\n",
                "\n",
                "# Register Trainers (Critical step!)\n",
                "import trainers.zsclip\n",
                "import trainers.coop\n",
                "import trainers.cocoop\n",
                "\n",
                "# Register Datasets (Critical step!)\n",
                "import datasets.oxford_pets\n",
                "import datasets.oxford_flowers\n",
                "import datasets.food101 # Added for Universal Transfer Test\n",
                "\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "\n",
                "# --- B) Load Surrogate (Victim) ---\n",
                "# We need to reconstruct the trainer/model structure to load the surrogate correctly\n",
                "class Args:\n",
                "    root = \"/content/data\"\n",
                "    dataset = \"oxford_pets\"\n",
                "    config_file = \"configs/trainers/CoOp/rn50.yaml\"\n",
                "    dataset_config_file = \"configs/datasets/oxford_pets.yaml\"\n",
                "    trainer = \"ZeroshotCLIP\"\n",
                "    head = \"ArcFace\"\n",
                "    output_dir = \"output\"\n",
                "    opts = []\n",
                "    gpu = 0\n",
                "    device = \"cuda:0\"\n",
                "    \n",
                "    # Missing attributes required by reset_cfg\n",
                "    resume = \"\"\n",
                "    seed = -1\n",
                "    source_domains = None\n",
                "    target_domains = None\n",
                "    transforms = None\n",
                "    backbone = \"\"\n",
                "    bs = 64\n",
                "    ratio = 1.0\n",
                "    \n",
                "args = Args()\n",
                "cfg = setup_cfg(args)\n",
                "trainer = build_trainer(cfg)\n",
                "\n",
                "# Manually build the surrogate model wrapper\n",
                "class Model(nn.Module):\n",
                "    def __init__(self, backbone, head_factory):\n",
                "        super(Model, self).__init__()\n",
                "        self.backbone = backbone\n",
                "        self.head = head_factory.get_head() # Fixed: Extract actual head module\n",
                "    def forward(self, x, labels=None):\n",
                "        feat = self.backbone(x)\n",
                "        return self.head(feat, labels)\n",
                "\n",
                "yaml_parser = YAML(typ='safe')\n",
                "config = yaml_parser.load(open('configs/data.yaml', 'r'))\n",
                "config['num_classes'] = trainer.dm.num_classes\n",
                "config['output_dim'] = 1024\n",
                "head_factory = HeadFactory(args.head, config)\n",
                "\n",
                "backbone = trainer.clip_model.visual\n",
                "# Wrap backbone with normalization\n",
                "normalize = transforms.Normalize([0.48145466, 0.4578275, 0.40821073], [0.26862954, 0.26130258, 0.27577711])\n",
                "backbone = nn.Sequential(normalize, backbone)\n",
                "\n",
                "surrogate = Model(backbone, head_factory).to(device)\n",
                "\n",
                "# Load weights\n",
                "surrogate_path = \"/content/data/oxford_pets/RN50_ArcFace.pth\"\n",
                "surrogate.load_state_dict(torch.load(surrogate_path, map_location=device))\n",
                "surrogate.eval()\n",
                "print(\"Surrogate Model Loaded.\")\n",
                "\n",
                "# --- C) Evaluation Loop ---\n",
                "from ignite.metrics import Accuracy\n",
                "from tqdm import tqdm\n",
                "import torch.nn.functional as F\n",
                "\n",
                "checkpoints = [\n",
                "    (\"/content/drive/MyDrive/grad/comp_vision/hanson_loss/oxford_pets/vit_generator.pt\", \"Targeted Only\"),\n",
                "    (\"/content/drive/MyDrive/grad/comp_vision/hanson_loss/oxford_pets/vit_generator_mixed_loss.pt\", \"Targeted + Contrastive\")\n",
                "]\n",
                "\n",
                "test_loader = trainer.test_loader\n",
                "eps = 16/255.0\n",
                "\n",
                "print(f\"\\nStarting Evaluation on {len(test_loader.dataset)} test images (Oxford Pets)...\")\n",
                "\n",
                "for ckpt_path, name in checkpoints:\n",
                "    print(f\"\\n>>> Evaluating: {name}\")\n",
                "    if not os.path.exists(ckpt_path):\n",
                "        print(f\"Skipping {name}: Checkpoint not found at {ckpt_path}\")\n",
                "        continue\n",
                "        \n",
                "    # Load Generator\n",
                "    generator = ViTGenerator(num_classes=37).to(device)\n",
                "    generator.load_state_dict(torch.load(ckpt_path, map_location=device), strict=False)\n",
                "    generator.eval()\n",
                "    \n",
                "    clean_acc_metric = Accuracy()\n",
                "    adv_acc_metric = Accuracy()\n",
                "    \n",
                "    for batch in tqdm(test_loader, desc=name):\n",
                "        images = batch['img'].to(device)\n",
                "        labels = batch['label'].to(device)\n",
                "        \n",
                "        # 1. Clean Accuracy\n",
                "        with torch.no_grad():\n",
                "            clean_outputs = surrogate(images, labels)\n",
                "            clean_acc_metric.update((clean_outputs, labels))\n",
                "            \n",
                "            # 2. Generate Adversarial Images (Targeted)\n",
                "            # Generate random targets != true labels to properly test targeted attack capability\n",
                "            target_labels = torch.randint(0, 37, labels.shape).to(device)\n",
                "            mask = (target_labels == labels)\n",
                "            target_labels[mask] = (target_labels[mask] + 1) % 37\n",
                "            \n",
                "            noise = generator(images, target_labels)\n",
                "            noise = torch.clamp(noise, -eps, eps)\n",
                "            adv_images = torch.clamp(images + noise, 0, 1)\n",
                "            \n",
                "            # 3. Adversarial Accuracy\n",
                "            adv_outputs = surrogate(adv_images, labels)\n",
                "            adv_acc_metric.update((adv_outputs, labels))\n",
                "\n",
                "    clean_acc = clean_acc_metric.compute()\n",
                "    adv_acc = adv_acc_metric.compute()\n",
                "    asr = clean_acc - adv_acc\n",
                "\n",
                "    print(f\"Results for {name}:\")\n",
                "    print(f\"  Clean Accuracy:       {clean_acc:.4f}\")\n",
                "    print(f\"  Adversarial Accuracy: {adv_acc:.4f}\")\n",
                "    print(f\"  Attack Success Rate:  {asr:.4f}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {
                "id": "eval_universal_transfer"
            },
            "outputs": [],
            "source": [
                "# 5) Universal Transfer Evaluation (Food101)\n",
                "# Goal: Does the Pet-Generator disrupt features on a completely different dataset (Food101)?\n",
                "# This tests if the \"Contrastive Loss\" learned a Universal Feature Disruptor.\n",
                "\n",
                "print(\"\\n\" + \"=\"*40)\n",
                "print(\"UNIVERSAL TRANSFER EVALUATION: Food101\")\n",
                "print(\"=\"*40)\n",
                "\n",
                "# 1. Setup Food101 Data\n",
                "# Note: Food101 is large, so we might just download it or assume it's there.\n",
                "# If download fails, we skip.\n",
                "try:\n",
                "    args_food = Args()\n",
                "    args_food.dataset = \"food101\"\n",
                "    args_food.dataset_config_file = \"configs/datasets/food101.yaml\"\n",
                "    cfg_food = setup_cfg(args_food)\n",
                "    trainer_food = build_trainer(cfg_food)\n",
                "    food_loader = trainer_food.test_loader\n",
                "    print(f\"Loaded Food101 with {len(food_loader.dataset)} test images.\")\n",
                "except Exception as e:\n",
                "    print(f\"Failed to load Food101: {e}\")\n",
                "    food_loader = None\n",
                "\n",
                "if food_loader:\n",
                "    # 2. Load Food101 Surrogate (Victim)\n",
                "    # We need to rebuild the surrogate model for Food101 (different num_classes)\n",
                "    \n",
                "    # Update config for Food101 classes\n",
                "    config_food = yaml_parser.load(open('configs/data.yaml', 'r'))\n",
                "    config_food['num_classes'] = trainer_food.dm.num_classes # Food101 classes\n",
                "    config_food['output_dim'] = 1024\n",
                "    head_factory_food = HeadFactory(args.head, config_food)\n",
                "    \n",
                "    surrogate_food = Model(backbone, head_factory_food).to(device)\n",
                "    \n",
                "    # Load Checkpoint\n",
                "    food_ckpt = \"/content/drive/MyDrive/grad/comp_vision/hanson_loss/oxford_pets/RN50_ArcFace_food101.pth\"\n",
                "    # Note: User's screenshot showed this file exists in the same folder\n",
                "    \n",
                "    if os.path.exists(food_ckpt):\n",
                "        surrogate_food.load_state_dict(torch.load(food_ckpt, map_location=device))\n",
                "        surrogate_food.eval()\n",
                "        print(\"Food101 Surrogate Loaded.\")\n",
                "        \n",
                "        # 3. Evaluate\n",
                "        for ckpt_path, name in checkpoints:\n",
                "            print(f\"\\n>>> Evaluating (Universal Transfer): {name}\")\n",
                "            if not os.path.exists(ckpt_path):\n",
                "                continue\n",
                "                \n",
                "            # Load Generator (Pet-Trained)\n",
                "            generator = ViTGenerator(num_classes=37).to(device)\n",
                "            generator.load_state_dict(torch.load(ckpt_path, map_location=device), strict=False)\n",
                "            generator.eval()\n",
                "            \n",
                "            clean_acc_metric = Accuracy()\n",
                "            adv_acc_metric = Accuracy()\n",
                "            \n",
                "            for batch in tqdm(food_loader, desc=name):\n",
                "                images = batch['img'].to(device)\n",
                "                labels = batch['label'].to(device)\n",
                "                \n",
                "                # 1. Clean Accuracy\n",
                "                with torch.no_grad():\n",
                "                    clean_outputs = surrogate_food(images, labels)\n",
                "                    clean_acc_metric.update((clean_outputs, labels))\n",
                "                    \n",
                "                    # 2. Generate Adversarial Images\n",
                "                    # We use random Pet targets (0-36) because the generator expects 37 classes\n",
                "                    # Ideally, for \"Universal Disruption\", the target shouldn't matter much\n",
                "                    target_labels = torch.randint(0, 37, (images.shape[0],)).to(device)\n",
                "                    \n",
                "                    noise = generator(images, target_labels)\n",
                "                    noise = torch.clamp(noise, -eps, eps)\n",
                "                    adv_images = torch.clamp(images + noise, 0, 1)\n",
                "                    \n",
                "                    # 3. Adversarial Accuracy (on Food101 Surrogate)\n",
                "                    adv_outputs = surrogate_food(adv_images, labels)\n",
                "                    adv_acc_metric.update((adv_outputs, labels))\n",
                "\n",
                "            clean_acc = clean_acc_metric.compute()\n",
                "            adv_acc = adv_acc_metric.compute()\n",
                "            asr = clean_acc - adv_acc\n",
                "\n",
                "            print(f\"Results for {name} on FOOD101:\")\n",
                "            print(f\"  Clean Accuracy:       {clean_acc:.4f}\")\n",
                "            print(f\"  Adversarial Accuracy: {adv_acc:.4f}\")\n",
                "            print(f\"  Attack Success Rate:  {asr:.4f}\")\n",
                "            print(\"  (Drop in Food101 accuracy caused by Pet-Generator)\")\n",
                "    else:\n",
                "        print(f\"Food101 Checkpoint not found at {food_ckpt}\")\n"
            ]
        }
    ]
}