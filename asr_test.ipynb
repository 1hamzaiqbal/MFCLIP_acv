{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "machine_shape": "hm",
            "gpuType": "L4",
            "include_colab_link": true
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "view-in-github"
            },
            "source": [
                "# Attack Success Rate (ASR) Evaluation\n",
                "\n",
                "This notebook evaluates the performance of a pre-trained ViT Generator on the **Test Set** of the Oxford Pets dataset.\n",
                "\n",
                "**Goals:**\n",
                "1.  Load the pre-trained (untargeted) generator.\n",
                "2.  Load the Surrogate Model (RN50).\n",
                "3.  Evaluate Clean Accuracy vs. Adversarial Accuracy on unseen Test Data.\n",
                "4.  Calculate ASR (Attack Success Rate)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "id": "mount_drive"
            },
            "outputs": [],
            "source": [
                "# 1) Mount Google Drive\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive', force_remount=True)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "id": "setup_repo"
            },
            "outputs": [],
            "source": [
                "# 2) Setup Repo & Dependencies\n",
                "!nvidia-smi\n",
                "%cd /content\n",
                "import os\n",
                "if not os.path.exists(\"MFCLIP_acv\"):\n",
                "    !git clone -b hamza/discrim https://github.com/1hamzaiqbal/MFCLIP_acv\n",
                "%cd MFCLIP_acv\n",
                "!git fetch --all\n",
                "!git reset --hard origin/hamza/discrim # Force sync to latest commit\n",
                "\n",
                "!pip install torch torchvision timm einops yacs tqdm opencv-python scikit-learn scipy pyyaml ruamel.yaml pytorch-ignite foolbox pandas matplotlib seaborn wilds ftfy\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "id": "setup_data"
            },
            "outputs": [],
            "source": [
                "# 3) Setup Data (Oxford Pets)\n",
                "import shutil\n",
                "import os\n",
                "from torchvision.datasets import OxfordIIITPet\n",
                "from torchvision import transforms\n",
                "from pathlib import Path\n",
                "\n",
                "# Download Dataset\n",
                "root = Path(\"/content/data/oxford_pets\")\n",
                "root.mkdir(parents=True, exist_ok=True)\n",
                "_ = OxfordIIITPet(root=str(root), download=True, transform=transforms.ToTensor())\n",
                "\n",
                "# Fetch Annotations\n",
                "%cd /content\n",
                "!mkdir -p /content/data/oxford_pets\n",
                "!wget -q https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n",
                "!wget -q https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\n",
                "!tar -xf images.tar.gz -C /content/data/oxford_pets\n",
                "!tar -xf annotations.tar.gz -C /content/data/oxford_pets\n",
                "\n",
                "# Copy Surrogate Checkpoint (Required for Evaluation)\n",
                "src_ckpt = \"/content/drive/MyDrive/grad/comp_vision/hanson_loss/oxford_pets/RN50_ArcFace_oxford_pets.pth\"\n",
                "dst_ckpt = \"/content/data/oxford_pets/RN50_ArcFace.pth\"\n",
                "\n",
                "if os.path.exists(src_ckpt):\n",
                "    shutil.copy(src_ckpt, dst_ckpt)\n",
                "    print(f\"Successfully copied surrogate checkpoint to {dst_ckpt}\")\n",
                "else:\n",
                "    raise FileNotFoundError(f\"Surrogate checkpoint not found at {src_ckpt}. Cannot evaluate without the victim model!\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "id": "load_models"
            },
            "outputs": [],
            "source": [
                "# 4) Load Models & Evaluate\n",
                "%cd /content/MFCLIP_acv\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "from model import ViTGenerator\n",
                "from utils.util import setup_cfg\n",
                "from dass.engine import build_trainer\n",
                "from loss.head.head_def import HeadFactory\n",
                "from torchvision import transforms\n",
                "from ruamel.yaml import YAML\n",
                "import argparse\n",
                "\n",
                "# Register Trainers (Critical step!)\n",
                "import trainers.zsclip\n",
                "import trainers.coop\n",
                "import trainers.cocoop\n",
                "\n",
                "# Register Datasets (Critical step!)\n",
                "import datasets.oxford_pets\n",
                "import datasets.oxford_flowers # Added for cross-dataset eval\n",
                "\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "\n",
                "# --- B) Load Surrogate (Victim) ---\n",
                "# We need to reconstruct the trainer/model structure to load the surrogate correctly\n",
                "class Args:\n",
                "    root = \"/content/data\"\n",
                "    dataset = \"oxford_pets\"\n",
                "    config_file = \"configs/trainers/CoOp/rn50.yaml\"\n",
                "    dataset_config_file = \"configs/datasets/oxford_pets.yaml\"\n",
                "    trainer = \"ZeroshotCLIP\"\n",
                "    head = \"ArcFace\"\n",
                "    output_dir = \"output\"\n",
                "    opts = []\n",
                "    gpu = 0\n",
                "    device = \"cuda:0\"\n",
                "    \n",
                "    # Missing attributes required by reset_cfg\n",
                "    resume = \"\"\n",
                "    seed = -1\n",
                "    source_domains = None\n",
                "    target_domains = None\n",
                "    transforms = None\n",
                "    backbone = \"\"\n",
                "    bs = 64\n",
                "    ratio = 1.0\n",
                "    \n",
                "args = Args()\n",
                "cfg = setup_cfg(args)\n",
                "trainer = build_trainer(cfg)\n",
                "\n",
                "# Manually build the surrogate model wrapper\n",
                "class Model(nn.Module):\n",
                "    def __init__(self, backbone, head_factory):\n",
                "        super(Model, self).__init__()\n",
                "        self.backbone = backbone\n",
                "        self.head = head_factory.get_head() # Fixed: Extract actual head module\n",
                "    def forward(self, x, labels=None):\n",
                "        feat = self.backbone(x)\n",
                "        return self.head(feat, labels)\n",
                "\n",
                "yaml_parser = YAML(typ='safe')\n",
                "config = yaml_parser.load(open('configs/data.yaml', 'r'))\n",
                "config['num_classes'] = trainer.dm.num_classes\n",
                "config['output_dim'] = 1024\n",
                "head_factory = HeadFactory(args.head, config)\n",
                "\n",
                "backbone = trainer.clip_model.visual\n",
                "# Wrap backbone with normalization\n",
                "normalize = transforms.Normalize([0.48145466, 0.4578275, 0.40821073], [0.26862954, 0.26130258, 0.27577711])\n",
                "backbone = nn.Sequential(normalize, backbone)\n",
                "\n",
                "surrogate = Model(backbone, head_factory).to(device)\n",
                "\n",
                "# Load weights\n",
                "surrogate_path = \"/content/data/oxford_pets/RN50_ArcFace.pth\"\n",
                "surrogate.load_state_dict(torch.load(surrogate_path, map_location=device))\n",
                "surrogate.eval()\n",
                "print(\"Surrogate Model Loaded.\")\n",
                "\n",
                "# --- C) Evaluation Loop ---\n",
                "from ignite.metrics import Accuracy\n",
                "from tqdm import tqdm\n",
                "import torch.nn.functional as F\n",
                "\n",
                "checkpoints = [\n",
                "    (\"/content/drive/MyDrive/grad/comp_vision/hanson_loss/oxford_pets/vit_generator.pt\", \"Targeted Only\"),\n",
                "    (\"/content/drive/MyDrive/grad/comp_vision/hanson_loss/oxford_pets/vit_generator_mixed_loss.pt\", \"Targeted + Contrastive\")\n",
                "]\n",
                "\n",
                "test_loader = trainer.test_loader\n",
                "eps = 16/255.0\n",
                "\n",
                "print(f\"\\nStarting Evaluation on {len(test_loader.dataset)} test images (Oxford Pets)...\")\n",
                "\n",
                "for ckpt_path, name in checkpoints:\n",
                "    print(f\"\\n>>> Evaluating: {name}\")\n",
                "    if not os.path.exists(ckpt_path):\n",
                "        print(f\"Skipping {name}: Checkpoint not found at {ckpt_path}\")\n",
                "        continue\n",
                "        \n",
                "    # Load Generator\n",
                "    generator = ViTGenerator(num_classes=37).to(device)\n",
                "    generator.load_state_dict(torch.load(ckpt_path, map_location=device), strict=False)\n",
                "    generator.eval()\n",
                "    \n",
                "    clean_acc_metric = Accuracy()\n",
                "    adv_acc_metric = Accuracy()\n",
                "    \n",
                "    for batch in tqdm(test_loader, desc=name):\n",
                "        images = batch['img'].to(device)\n",
                "        labels = batch['label'].to(device)\n",
                "        \n",
                "        # 1. Clean Accuracy\n",
                "        with torch.no_grad():\n",
                "            clean_outputs = surrogate(images, labels)\n",
                "            clean_acc_metric.update((clean_outputs, labels))\n",
                "            \n",
                "            # 2. Generate Adversarial Images (Targeted)\n",
                "            # Generate random targets != true labels to properly test targeted attack capability\n",
                "            target_labels = torch.randint(0, 37, labels.shape).to(device)\n",
                "            mask = (target_labels == labels)\n",
                "            target_labels[mask] = (target_labels[mask] + 1) % 37\n",
                "            \n",
                "            noise = generator(images, target_labels)\n",
                "            noise = torch.clamp(noise, -eps, eps)\n",
                "            adv_images = torch.clamp(images + noise, 0, 1)\n",
                "            \n",
                "            # 3. Adversarial Accuracy\n",
                "            adv_outputs = surrogate(adv_images, labels)\n",
                "            adv_acc_metric.update((adv_outputs, labels))\n",
                "\n",
                "    clean_acc = clean_acc_metric.compute()\n",
                "    adv_acc = adv_acc_metric.compute()\n",
                "    asr = clean_acc - adv_acc\n",
                "\n",
                "    print(f\"Results for {name}:\")\n",
                "    print(f\"  Clean Accuracy:       {clean_acc:.4f}\")\n",
                "    print(f\"  Adversarial Accuracy: {adv_acc:.4f}\")\n",
                "    print(f\"  Attack Success Rate:  {asr:.4f}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {
                "id": "eval_cross_dataset"
            },
            "outputs": [],
            "source": [
                "# 5) Cross-Dataset Evaluation (Oxford Flowers -> Oxford Pets Surrogate)\n",
                "# Goal: Can we force the Pet Classifier to see a specific Pet in a Flower image?\n",
                "\n",
                "print(\"\\n\" + \"=\"*40)\n",
                "print(\"CROSS-DATASET EVALUATION: Oxford Flowers\")\n",
                "print(\"=\"*40)\n",
                "\n",
                "# Setup Oxford Flowers Data\n",
                "from torchvision.datasets import Oxford102\n",
                "root_flowers = Path(\"/content/data/oxford_flowers\")\n",
                "root_flowers.mkdir(parents=True, exist_ok=True)\n",
                "# Note: Oxford102 download might need manual handling if URL is broken, but let's try standard torchvision\n",
                "# If torchvision fails, we use the repo's dataset loader\n",
                "\n",
                "# Use the Repo's Dataset Loader for Flowers\n",
                "args_flowers = Args()\n",
                "args_flowers.dataset = \"oxford_flowers\"\n",
                "args_flowers.dataset_config_file = \"configs/datasets/oxford_flowers.yaml\"\n",
                "cfg_flowers = setup_cfg(args_flowers)\n",
                "trainer_flowers = build_trainer(cfg_flowers)\n",
                "flowers_loader = trainer_flowers.test_loader\n",
                "\n",
                "print(f\"Evaluating on {len(flowers_loader.dataset)} Flower images...\")\n",
                "\n",
                "for ckpt_path, name in checkpoints:\n",
                "    print(f\"\\n>>> Evaluating (Cross-Dataset): {name}\")\n",
                "    if not os.path.exists(ckpt_path):\n",
                "        continue\n",
                "        \n",
                "    # Load Generator\n",
                "    generator = ViTGenerator(num_classes=37).to(device)\n",
                "    generator.load_state_dict(torch.load(ckpt_path, map_location=device), strict=False)\n",
                "    generator.eval()\n",
                "    \n",
                "    # Metrics\n",
                "    # Note: Clean Acc is meaningless here (Flowers labeled as Pets?)\n",
                "    # But we want to see if we can force the Target Label.\n",
                "    # So 'Adversarial Accuracy' = How often does Surrogate predict Target Label?\n",
                "    \n",
                "    target_success_metric = Accuracy()\n",
                "    \n",
                "    for batch in tqdm(flowers_loader, desc=name):\n",
                "        images = batch['img'].to(device)\n",
                "        # Ignore true labels (they are flower classes)\n",
                "        \n",
                "        # Generate Random Target Pet Labels\n",
                "        target_labels = torch.randint(0, 37, (images.shape[0],)).to(device)\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            noise = generator(images, target_labels)\n",
                "            noise = torch.clamp(noise, -eps, eps)\n",
                "            adv_images = torch.clamp(images + noise, 0, 1)\n",
                "            \n",
                "            adv_outputs = surrogate(adv_images)\n",
                "            \n",
                "            # Check if prediction matches target\n",
                "            target_success_metric.update((adv_outputs, target_labels))\n",
                "\n",
                "    success_rate = target_success_metric.compute()\n",
                "    print(f\"Results for {name} on FLOWERS:\")\n",
                "    print(f\"  Targeted Attack Success Rate: {success_rate:.4f}\")\n",
                "    print(\"  (Fraction of Flower images successfully disguised as the target Pet)\")\n"
            ]
        }
    ]
}