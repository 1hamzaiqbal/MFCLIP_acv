{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ViT Generator Evaluation - ASR Test\n",
        "\n",
        "This notebook evaluates the 3 ViT generator variants on:\n",
        "\n",
        "1. **Oxford Pets** (same dataset as training) - measures in-domain ASR\n",
        "2. **Food101** (different dataset) - measures cross-domain transfer/generalizability\n",
        "\n",
        "**Metrics:**\n",
        "- **Clean Accuracy**: Surrogate accuracy on clean images\n",
        "- **Adversarial Accuracy**: Surrogate accuracy on adversarial images  \n",
        "- **Attack Success Rate (ASR)**: Clean Acc - Adv Acc (higher = better attack)\n",
        "- **Targeted Success Rate**: How often the model predicts the target class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import os\n",
        "DRIVE_OUTPUT = \"/content/drive/MyDrive/grad/comp_vision/hanson_loss\"\n",
        "print(f\"Checkpoint directory: {DRIVE_OUTPUT}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: Setup Repository & Dependencies\n",
        "!nvidia-smi\n",
        "%cd /content\n",
        "\n",
        "import os\n",
        "if not os.path.exists(\"MFCLIP_acv\"):\n",
        "    !git clone -b hamza/discrim https://github.com/1hamzaiqbal/MFCLIP_acv\n",
        "\n",
        "%cd MFCLIP_acv\n",
        "!git fetch --all\n",
        "!git reset --hard origin/hamza/discrim\n",
        "\n",
        "# Install dependencies\n",
        "!pip install -q torch torchvision timm einops yacs tqdm opencv-python \\\n",
        "    scikit-learn scipy pyyaml ruamel.yaml pytorch-ignite foolbox \\\n",
        "    pandas matplotlib seaborn wilds ftfy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Setup Oxford Pets Dataset\n",
        "import shutil\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_ROOT = \"/content/data\"\n",
        "PETS_ROOT = f\"{DATA_ROOT}/oxford_pets\"\n",
        "DRIVE_OUTPUT = \"/content/drive/MyDrive/grad/comp_vision/hanson_loss\"\n",
        "\n",
        "Path(PETS_ROOT).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "%cd /content\n",
        "if not os.path.exists(f\"{PETS_ROOT}/images\"):\n",
        "    print(\"Downloading Oxford Pets dataset...\")\n",
        "    !wget -q https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n",
        "    !wget -q https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\n",
        "    !tar -xf images.tar.gz -C {PETS_ROOT}\n",
        "    !tar -xf annotations.tar.gz -C {PETS_ROOT}\n",
        "    !rm -f images.tar.gz annotations.tar.gz\n",
        "    print(\"Dataset ready!\")\n",
        "else:\n",
        "    print(\"Oxford Pets dataset already exists.\")\n",
        "\n",
        "# Copy surrogate checkpoint\n",
        "SURROGATE_SRC = f\"{DRIVE_OUTPUT}/oxford_pets/RN50_ArcFace_oxford_pets.pth\"\n",
        "SURROGATE_DST = f\"{PETS_ROOT}/RN50_ArcFace.pth\"\n",
        "if os.path.exists(SURROGATE_SRC):\n",
        "    shutil.copy(SURROGATE_SRC, SURROGATE_DST)\n",
        "    print(f\"Surrogate checkpoint ready: {SURROGATE_DST}\")\n",
        "else:\n",
        "    print(f\"WARNING: Surrogate not found at {SURROGATE_SRC}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Copy Generator Checkpoints from Drive\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "DRIVE_OUTPUT = \"/content/drive/MyDrive/grad/comp_vision/hanson_loss/oxford_pets\"\n",
        "PETS_ROOT = \"/content/data/oxford_pets\"\n",
        "\n",
        "checkpoints = [\n",
        "    \"vit_generator_targeted_only.pt\",\n",
        "    \"vit_generator_contrastive.pt\", \n",
        "    \"vit_generator_mixed.pt\",\n",
        "]\n",
        "\n",
        "print(\"Copying generator checkpoints from Drive...\")\n",
        "for ckpt in checkpoints:\n",
        "    src = f\"{DRIVE_OUTPUT}/{ckpt}\"\n",
        "    dst = f\"{PETS_ROOT}/{ckpt}\"\n",
        "    if os.path.exists(src):\n",
        "        shutil.copy(src, dst)\n",
        "        print(f\"  âœ“ {ckpt}\")\n",
        "    else:\n",
        "        print(f\"  âœ— {ckpt} (not found in Drive)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Load Models and Setup Evaluation Infrastructure\n",
        "%cd /content/MFCLIP_acv\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from ignite.metrics import Accuracy\n",
        "from ruamel.yaml import YAML\n",
        "import sys\n",
        "sys.path.insert(0, '/content/MFCLIP_acv')\n",
        "\n",
        "from model import ViTGenerator\n",
        "from utils.util import setup_cfg, Model\n",
        "from dass.engine import build_trainer\n",
        "from loss.head.head_def import HeadFactory\n",
        "from torchvision import transforms\n",
        "import argparse\n",
        "\n",
        "# Register trainers and datasets\n",
        "import trainers.zsclip\n",
        "import trainers.coop\n",
        "import trainers.cocoop\n",
        "import datasets.oxford_pets\n",
        "import datasets.oxford_flowers\n",
        "import datasets.food101\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Build trainer for Oxford Pets\n",
        "class Args:\n",
        "    root = \"/content/data\"\n",
        "    dataset = \"oxford_pets\"\n",
        "    config_file = \"configs/trainers/CoOp/rn50.yaml\"\n",
        "    dataset_config_file = \"configs/datasets/oxford_pets.yaml\"\n",
        "    trainer = \"ZeroshotCLIP\"\n",
        "    head = \"ArcFace\"\n",
        "    output_dir = \"output\"\n",
        "    opts = []\n",
        "    gpu = 0\n",
        "    device = \"cuda:0\"\n",
        "    resume = \"\"\n",
        "    seed = -1\n",
        "    source_domains = None\n",
        "    target_domains = None\n",
        "    transforms = None\n",
        "    backbone = \"\"\n",
        "    bs = 64\n",
        "    ratio = 1.0\n",
        "\n",
        "args = Args()\n",
        "cfg = setup_cfg(args)\n",
        "trainer = build_trainer(cfg)\n",
        "\n",
        "# Build Surrogate Model\n",
        "yaml_parser = YAML(typ='safe')\n",
        "config = yaml_parser.load(open('configs/data.yaml', 'r'))\n",
        "config['num_classes'] = trainer.dm.num_classes\n",
        "config['output_dim'] = 1024\n",
        "head_factory = HeadFactory(args.head, config)\n",
        "\n",
        "backbone = trainer.clip_model.visual\n",
        "normalize = transforms.Normalize([0.48145466, 0.4578275, 0.40821073], [0.26862954, 0.26130258, 0.27577711])\n",
        "backbone = nn.Sequential(normalize, backbone)\n",
        "\n",
        "surrogate = Model(backbone, head_factory).to(device)\n",
        "\n",
        "# Load surrogate weights\n",
        "surrogate_path = \"/content/data/oxford_pets/RN50_ArcFace.pth\"\n",
        "surrogate.load_state_dict(torch.load(surrogate_path, map_location=device))\n",
        "surrogate.eval()\n",
        "print(\"âœ“ Surrogate Model Loaded\")\n",
        "\n",
        "# Get test loader\n",
        "test_loader = trainer.test_loader\n",
        "print(f\"âœ“ Test set: {len(test_loader.dataset)} samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation Function\n",
        "\n",
        "This function evaluates a generator on a given dataset and computes ASR metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: Evaluation Function\n",
        "def evaluate_generator(generator, surrogate, test_loader, num_classes, eps=16/255., device='cuda'):\n",
        "    \"\"\"\n",
        "    Evaluate a generator on a test loader.\n",
        "    \n",
        "    Returns:\n",
        "        dict with clean_acc, adv_acc, asr, targeted_success_rate\n",
        "    \"\"\"\n",
        "    generator.eval()\n",
        "    surrogate.eval()\n",
        "    \n",
        "    clean_acc_metric = Accuracy()\n",
        "    adv_acc_metric = Accuracy()\n",
        "    targeted_success_metric = Accuracy()\n",
        "    \n",
        "    for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "        images = batch['img'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "        \n",
        "        # Generate random target labels != true labels\n",
        "        target_labels = torch.randint(0, num_classes, labels.shape).to(device)\n",
        "        mask = (target_labels == labels)\n",
        "        target_labels[mask] = (target_labels[mask] + 1) % num_classes\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            # Clean accuracy\n",
        "            clean_outputs = surrogate(images, labels)\n",
        "            clean_acc_metric.update((clean_outputs, labels))\n",
        "            \n",
        "            # Generate adversarial images\n",
        "            noise = generator(images, target_labels)\n",
        "            noise = torch.clamp(noise, -eps, eps)\n",
        "            adv_images = torch.clamp(images + noise, 0, 1)\n",
        "            \n",
        "            # Adversarial accuracy\n",
        "            adv_outputs = surrogate(adv_images, labels)\n",
        "            adv_acc_metric.update((adv_outputs, labels))\n",
        "            \n",
        "            # Targeted success (does model predict target class?)\n",
        "            targeted_success_metric.update((adv_outputs, target_labels))\n",
        "    \n",
        "    clean_acc = clean_acc_metric.compute()\n",
        "    adv_acc = adv_acc_metric.compute()\n",
        "    asr = clean_acc - adv_acc\n",
        "    targeted_success = targeted_success_metric.compute()\n",
        "    \n",
        "    return {\n",
        "        'clean_acc': clean_acc,\n",
        "        'adv_acc': adv_acc,\n",
        "        'asr': asr,\n",
        "        'targeted_success': targeted_success,\n",
        "    }\n",
        "\n",
        "print(\"âœ“ Evaluation function defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: Evaluate All Variants on Oxford Pets\n",
        "import pandas as pd\n",
        "\n",
        "PETS_ROOT = \"/content/data/oxford_pets\"\n",
        "NUM_CLASSES = 37  # Oxford Pets has 37 classes\n",
        "EPS = 16/255.\n",
        "\n",
        "variants = [\n",
        "    (\"targeted_only\", f\"{PETS_ROOT}/vit_generator_targeted_only.pt\"),\n",
        "    (\"contrastive\", f\"{PETS_ROOT}/vit_generator_contrastive.pt\"),\n",
        "    (\"mixed\", f\"{PETS_ROOT}/vit_generator_mixed.pt\"),\n",
        "]\n",
        "\n",
        "results_pets = []\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"OXFORD PETS EVALUATION (In-Domain)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for name, ckpt_path in variants:\n",
        "    print(f\"\\n>>> Evaluating: {name}\")\n",
        "    \n",
        "    if not os.path.exists(ckpt_path):\n",
        "        print(f\"  âœ— Checkpoint not found: {ckpt_path}\")\n",
        "        results_pets.append({'variant': name, 'clean_acc': None, 'adv_acc': None, 'asr': None, 'targeted_success': None})\n",
        "        continue\n",
        "    \n",
        "    # Load generator\n",
        "    generator = ViTGenerator(num_classes=NUM_CLASSES).to(device)\n",
        "    generator.load_state_dict(torch.load(ckpt_path, map_location=device), strict=False)\n",
        "    generator.eval()\n",
        "    \n",
        "    # Evaluate\n",
        "    metrics = evaluate_generator(generator, surrogate, test_loader, NUM_CLASSES, EPS, device)\n",
        "    \n",
        "    print(f\"  Clean Accuracy:       {metrics['clean_acc']:.4f}\")\n",
        "    print(f\"  Adversarial Accuracy: {metrics['adv_acc']:.4f}\")\n",
        "    print(f\"  Attack Success Rate:  {metrics['asr']:.4f}\")\n",
        "    print(f\"  Targeted Success:     {metrics['targeted_success']:.4f}\")\n",
        "    \n",
        "    results_pets.append({\n",
        "        'variant': name,\n",
        "        'clean_acc': metrics['clean_acc'],\n",
        "        'adv_acc': metrics['adv_acc'],\n",
        "        'asr': metrics['asr'],\n",
        "        'targeted_success': metrics['targeted_success'],\n",
        "    })\n",
        "    \n",
        "    # Free memory\n",
        "    del generator\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Display results table\n",
        "df_pets = pd.DataFrame(results_pets)\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"OXFORD PETS RESULTS SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(df_pets.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup Food101 for Cross-Domain Evaluation\n",
        "\n",
        "Download and prepare Food101 dataset, then build a new surrogate trained on Food101.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 8: Setup Food101 Dataset\n",
        "import os\n",
        "\n",
        "FOOD_ROOT = \"/content/data/food-101\"\n",
        "\n",
        "print(\"Checking Food101 dataset...\")\n",
        "if not os.path.exists(f\"{FOOD_ROOT}/images\"):\n",
        "    print(\"Downloading Food101 (approx 5GB)... This might take a few minutes.\")\n",
        "    %cd /content/data\n",
        "    !wget -q http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz\n",
        "    print(\"Extracting Food101...\")\n",
        "    !tar -xzf food-101.tar.gz\n",
        "    !rm -f food-101.tar.gz\n",
        "    print(\"âœ“ Food101 Ready!\")\n",
        "else:\n",
        "    print(\"âœ“ Food101 dataset already exists.\")\n",
        "\n",
        "# Setup Food101 trainer\n",
        "args_food = Args()\n",
        "args_food.dataset = \"food101\"\n",
        "args_food.dataset_config_file = \"configs/datasets/food101.yaml\"\n",
        "\n",
        "%cd /content/MFCLIP_acv\n",
        "cfg_food = setup_cfg(args_food)\n",
        "trainer_food = build_trainer(cfg_food)\n",
        "food_loader = trainer_food.test_loader\n",
        "food_num_classes = trainer_food.dm.num_classes\n",
        "\n",
        "print(f\"âœ“ Food101 test set: {len(food_loader.dataset)} samples\")\n",
        "print(f\"âœ“ Food101 classes: {food_num_classes}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate on Food101 (Cross-Domain Transfer)\n",
        "\n",
        "Test how well the Pet-trained generators transfer to a completely different domain (food images).\n",
        "This measures the generalizability of the learned perturbations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 10: Evaluate Cross-Domain Transfer on Food101\n",
        "PETS_ROOT = \"/content/data/oxford_pets\"\n",
        "PETS_NUM_CLASSES = 37\n",
        "\n",
        "def evaluate_cross_domain(generator, surrogate_target, test_loader, generator_num_classes, target_num_classes, eps=16/255., device='cuda'):\n",
        "    \"\"\"\n",
        "    Evaluate a generator (trained on one domain) on a different domain.\n",
        "    Generator uses its original num_classes for target generation.\n",
        "    \"\"\"\n",
        "    generator.eval()\n",
        "    surrogate_target.eval()\n",
        "    \n",
        "    clean_acc_metric = Accuracy()\n",
        "    adv_acc_metric = Accuracy()\n",
        "    \n",
        "    for batch in tqdm(test_loader, desc=\"Cross-Domain Eval\"):\n",
        "        images = batch['img'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "        \n",
        "        # Use generator's class space for targets (e.g., Pet classes 0-36)\n",
        "        target_labels = torch.randint(0, generator_num_classes, labels.shape).to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            # Clean accuracy on target domain\n",
        "            clean_outputs = surrogate_target(images, labels)\n",
        "            clean_acc_metric.update((clean_outputs, labels))\n",
        "            \n",
        "            # Generate adversarial images using Pet-trained generator\n",
        "            noise = generator(images, target_labels)\n",
        "            noise = torch.clamp(noise, -eps, eps)\n",
        "            adv_images = torch.clamp(images + noise, 0, 1)\n",
        "            \n",
        "            # Adversarial accuracy on target domain\n",
        "            adv_outputs = surrogate_target(adv_images, labels)\n",
        "            adv_acc_metric.update((adv_outputs, labels))\n",
        "    \n",
        "    clean_acc = clean_acc_metric.compute()\n",
        "    adv_acc = adv_acc_metric.compute()\n",
        "    asr = clean_acc - adv_acc\n",
        "    \n",
        "    return {\n",
        "        'clean_acc': clean_acc,\n",
        "        'adv_acc': adv_acc,\n",
        "        'asr': asr,\n",
        "    }\n",
        "\n",
        "# Only run if Food101 surrogate is available\n",
        "if FOOD_SURROGATE_AVAILABLE:\n",
        "    results_food = []\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    print(\"FOOD101 EVALUATION (Cross-Domain Transfer)\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Using Pet-trained generators on Food images\\n\")\n",
        "    \n",
        "    for name, ckpt_path in variants:\n",
        "        print(f\"\\n>>> Evaluating: {name}\")\n",
        "        \n",
        "        if not os.path.exists(ckpt_path):\n",
        "            print(f\"  âœ— Checkpoint not found\")\n",
        "            results_food.append({'variant': name, 'clean_acc': None, 'adv_acc': None, 'asr': None})\n",
        "            continue\n",
        "        \n",
        "        # Load generator (trained on Pets with 37 classes)\n",
        "        generator = ViTGenerator(num_classes=PETS_NUM_CLASSES).to(device)\n",
        "        generator.load_state_dict(torch.load(ckpt_path, map_location=device), strict=False)\n",
        "        generator.eval()\n",
        "        \n",
        "        # Evaluate cross-domain\n",
        "        metrics = evaluate_cross_domain(\n",
        "            generator, surrogate_food, food_loader,\n",
        "            generator_num_classes=PETS_NUM_CLASSES,\n",
        "            target_num_classes=food_num_classes,\n",
        "            eps=EPS, device=device\n",
        "        )\n",
        "        \n",
        "        print(f\"  Clean Accuracy:       {metrics['clean_acc']:.4f}\")\n",
        "        print(f\"  Adversarial Accuracy: {metrics['adv_acc']:.4f}\")\n",
        "        print(f\"  Attack Success Rate:  {metrics['asr']:.4f}\")\n",
        "        \n",
        "        results_food.append({\n",
        "            'variant': name,\n",
        "            'clean_acc': metrics['clean_acc'],\n",
        "            'adv_acc': metrics['adv_acc'],\n",
        "            'asr': metrics['asr'],\n",
        "        })\n",
        "        \n",
        "        del generator\n",
        "        torch.cuda.empty_cache()\n",
        "    \n",
        "    # Display results\n",
        "    df_food = pd.DataFrame(results_food)\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"FOOD101 RESULTS SUMMARY\")\n",
        "    print(\"=\" * 60)\n",
        "    print(df_food.to_string(index=False))\n",
        "else:\n",
        "    print(\"Skipping Food101 evaluation - surrogate not available.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization: Perturbation Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 11: Visualize Perturbations from All Variants\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import glob\n",
        "\n",
        "PETS_ROOT = \"/content/data/oxford_pets\"\n",
        "\n",
        "# Get a sample image\n",
        "images = glob.glob(f\"{PETS_ROOT}/images/*.jpg\")\n",
        "if images:\n",
        "    img_path = images[0]\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    img_t = transform(img).unsqueeze(0).to(device)\n",
        "    \n",
        "    target_label = torch.tensor([5]).to(device)\n",
        "    eps = 16/255\n",
        "    \n",
        "    fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
        "    \n",
        "    def show_tensor(t, ax, title):\n",
        "        im = t.squeeze().cpu().permute(1, 2, 0).numpy()\n",
        "        im = np.clip(im, 0, 1)\n",
        "        ax.imshow(im)\n",
        "        ax.set_title(title, fontsize=10)\n",
        "        ax.axis('off')\n",
        "    \n",
        "    for row, (name, ckpt_path) in enumerate(variants):\n",
        "        if not os.path.exists(ckpt_path):\n",
        "            for col in range(4):\n",
        "                axes[row, col].axis('off')\n",
        "                axes[row, col].set_title(f\"{name}: Not Found\")\n",
        "            continue\n",
        "        \n",
        "        gen = ViTGenerator(num_classes=37).to(device)\n",
        "        gen.load_state_dict(torch.load(ckpt_path, map_location=device), strict=False)\n",
        "        gen.eval()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            noise = gen(img_t, target_label)\n",
        "            noise = torch.clamp(noise, -eps, eps)\n",
        "            adv = torch.clamp(img_t + noise, 0, 1)\n",
        "        \n",
        "        # Original\n",
        "        show_tensor(img_t, axes[row, 0], f\"Original\")\n",
        "        \n",
        "        # Perturbation (normalized for visibility)\n",
        "        noise_np = noise.squeeze().cpu().permute(1, 2, 0).numpy()\n",
        "        noise_vis = (noise_np - noise_np.min()) / (noise_np.max() - noise_np.min() + 1e-8)\n",
        "        axes[row, 1].imshow(noise_vis)\n",
        "        axes[row, 1].set_title(f\"{name}: Perturbation\")\n",
        "        axes[row, 1].axis('off')\n",
        "        \n",
        "        # Adversarial\n",
        "        show_tensor(adv, axes[row, 2], f\"{name}: Adversarial\")\n",
        "        \n",
        "        # Difference (amplified)\n",
        "        diff = (adv - img_t).abs()\n",
        "        diff_np = diff.squeeze().cpu().permute(1, 2, 0).numpy()\n",
        "        diff_vis = diff_np * 10  # Amplify for visibility\n",
        "        diff_vis = np.clip(diff_vis, 0, 1)\n",
        "        axes[row, 3].imshow(diff_vis)\n",
        "        axes[row, 3].set_title(f\"{name}: Diff (10x)\")\n",
        "        axes[row, 3].axis('off')\n",
        "        \n",
        "        del gen\n",
        "        torch.cuda.empty_cache()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{PETS_ROOT}/perturbation_comparison.png\", dpi=150)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No images found for visualization.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final Summary\n",
        "\n",
        "Comparison of all variants across both datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 12: Final Summary\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"FINAL RESULTS SUMMARY\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\nðŸ“Š OXFORD PETS (In-Domain):\")\n",
        "print(df_pets.to_string(index=False))\n",
        "\n",
        "if FOOD_SURROGATE_AVAILABLE:\n",
        "    print(\"\\nðŸ“Š FOOD101 (Cross-Domain Transfer):\")\n",
        "    print(df_food.to_string(index=False))\n",
        "\n",
        "# Plot comparison bar chart\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "variants_names = [v[0] for v in variants]\n",
        "colors = ['#2196F3', '#F44336', '#4CAF50']\n",
        "\n",
        "# Plot 1: Oxford Pets ASR\n",
        "ax = axes[0]\n",
        "asr_pets = [r['asr'] if r['asr'] is not None else 0 for r in results_pets]\n",
        "bars = ax.bar(variants_names, asr_pets, color=colors)\n",
        "ax.set_ylabel('Attack Success Rate')\n",
        "ax.set_title('Oxford Pets (In-Domain)')\n",
        "ax.set_ylim(0, 1)\n",
        "for bar, val in zip(bars, asr_pets):\n",
        "    if val > 0:\n",
        "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
        "                f'{val:.3f}', ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "# Plot 2: Food101 ASR (if available)\n",
        "ax = axes[1]\n",
        "if FOOD_SURROGATE_AVAILABLE:\n",
        "    asr_food = [r['asr'] if r['asr'] is not None else 0 for r in results_food]\n",
        "    bars = ax.bar(variants_names, asr_food, color=colors)\n",
        "    ax.set_ylabel('Attack Success Rate')\n",
        "    ax.set_title('Food101 (Cross-Domain Transfer)')\n",
        "    ax.set_ylim(0, 1)\n",
        "    for bar, val in zip(bars, asr_food):\n",
        "        if val > 0:\n",
        "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
        "                    f'{val:.3f}', ha='center', va='bottom', fontsize=10)\n",
        "else:\n",
        "    ax.text(0.5, 0.5, 'Food101 Surrogate\\nNot Available', \n",
        "            ha='center', va='center', fontsize=14, transform=ax.transAxes)\n",
        "    ax.set_title('Food101 (Cross-Domain Transfer)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{PETS_ROOT}/asr_comparison.png\", dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nResults saved to {PETS_ROOT}/\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 9: Build Food101 Surrogate (if checkpoint exists)\n",
        "DRIVE_OUTPUT = \"/content/drive/MyDrive/grad/comp_vision/hanson_loss\"\n",
        "FOOD_SURROGATE_SRC = f\"{DRIVE_OUTPUT}/food101/RN50_ArcFace_food101.pth\"\n",
        "\n",
        "# Build Food101 surrogate model\n",
        "config_food = yaml_parser.load(open('configs/data.yaml', 'r'))\n",
        "config_food['num_classes'] = food_num_classes\n",
        "config_food['output_dim'] = 1024\n",
        "head_factory_food = HeadFactory(args.head, config_food)\n",
        "\n",
        "surrogate_food = Model(backbone, head_factory_food).to(device)\n",
        "\n",
        "if os.path.exists(FOOD_SURROGATE_SRC):\n",
        "    surrogate_food.load_state_dict(torch.load(FOOD_SURROGATE_SRC, map_location=device))\n",
        "    surrogate_food.eval()\n",
        "    print(f\"âœ“ Food101 Surrogate loaded from {FOOD_SURROGATE_SRC}\")\n",
        "    FOOD_SURROGATE_AVAILABLE = True\n",
        "else:\n",
        "    print(f\"âœ— Food101 Surrogate not found at {FOOD_SURROGATE_SRC}\")\n",
        "    print(\"  Cross-domain evaluation will measure feature disruption only.\")\n",
        "    FOOD_SURROGATE_AVAILABLE = False\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
