{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ViT Generator Training - Clean Notebook\n",
        "\n",
        "This notebook trains 3 variants of the ViT-based adversarial generator:\n",
        "\n",
        "1. **targeted_only**: Targeted attack loss only (minimize loss for target class)\n",
        "2. **contrastive**: Targeted + Full contrastive loss (maximize feature distance)\n",
        "3. **mixed**: Targeted + Weighted contrastive loss (adjustable weight)\n",
        "\n",
        "**Key Fixes Applied:**\n",
        "- Uses TRAIN set (not test set) for training\n",
        "- Reduced batch size to avoid OOM on 22.5GB VRAM\n",
        "- Proper checkpoint saving to Google Drive\n",
        "- Training history tracking and visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Create output directory structure\n",
        "import os\n",
        "DRIVE_OUTPUT = \"/content/drive/MyDrive/grad/comp_vision/hanson_loss\"\n",
        "os.makedirs(f\"{DRIVE_OUTPUT}/oxford_pets\", exist_ok=True)\n",
        "os.makedirs(f\"{DRIVE_OUTPUT}/food101\", exist_ok=True)\n",
        "print(f\"Output directory: {DRIVE_OUTPUT}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: Setup Repository & Dependencies\n",
        "!nvidia-smi\n",
        "%cd /content\n",
        "\n",
        "import os\n",
        "if not os.path.exists(\"MFCLIP_acv\"):\n",
        "    !git clone -b hamza/discrim https://github.com/1hamzaiqbal/MFCLIP_acv\n",
        "\n",
        "%cd MFCLIP_acv\n",
        "!git fetch --all\n",
        "!git reset --hard origin/hamza/discrim\n",
        "\n",
        "# Install dependencies\n",
        "!pip install -q torch torchvision timm einops yacs tqdm opencv-python \\\n",
        "    scikit-learn scipy pyyaml ruamel.yaml pytorch-ignite foolbox \\\n",
        "    pandas matplotlib seaborn wilds ftfy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Setup Oxford Pets Dataset & Surrogate Checkpoint\n",
        "import shutil\n",
        "import os\n",
        "from pathlib import Path\n",
        "from torchvision.datasets import OxfordIIITPet\n",
        "from torchvision import transforms\n",
        "\n",
        "DATA_ROOT = \"/content/data\"\n",
        "PETS_ROOT = f\"{DATA_ROOT}/oxford_pets\"\n",
        "DRIVE_OUTPUT = \"/content/drive/MyDrive/grad/comp_vision/hanson_loss\"\n",
        "\n",
        "# Create directories\n",
        "Path(PETS_ROOT).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Download dataset via torchvision (creates split files)\n",
        "print(\"Setting up Oxford Pets dataset...\")\n",
        "_ = OxfordIIITPet(root=PETS_ROOT, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "# Also fetch raw images and annotations (needed by dataloader)\n",
        "%cd /content\n",
        "if not os.path.exists(f\"{PETS_ROOT}/images\"):\n",
        "    print(\"Downloading images and annotations...\")\n",
        "    !wget -q https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n",
        "    !wget -q https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\n",
        "    !tar -xf images.tar.gz -C {PETS_ROOT}\n",
        "    !tar -xf annotations.tar.gz -C {PETS_ROOT}\n",
        "    !rm -f images.tar.gz annotations.tar.gz\n",
        "print(\"✓ Dataset ready!\")\n",
        "\n",
        "# Copy pre-trained RN50 surrogate checkpoint from Drive\n",
        "SURROGATE_SRC = f\"{DRIVE_OUTPUT}/oxford_pets/RN50_ArcFace_oxford_pets.pth\"\n",
        "SURROGATE_DST = f\"{PETS_ROOT}/RN50_ArcFace.pth\"\n",
        "\n",
        "if os.path.exists(SURROGATE_SRC):\n",
        "    shutil.copy(SURROGATE_SRC, SURROGATE_DST)\n",
        "    print(f\"✓ Surrogate checkpoint copied to {SURROGATE_DST}\")\n",
        "else:\n",
        "    print(f\"⚠️  WARNING: Surrogate not found at {SURROGATE_SRC}\")\n",
        "    print(\"   You need to train the surrogate first using finetune mode.\")\n",
        "    print(\"   Or copy your existing RN50_ArcFace checkpoint to Google Drive.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Configuration\n",
        "\n",
        "Adjust batch size if you get OOM errors. The default of 32 should work on 22.5GB VRAM.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Training Configuration\n",
        "CONFIG = {\n",
        "    'dataset': 'oxford_pets',\n",
        "    'num_epochs': 150,       # Reduced for faster training\n",
        "    'batch_size': 32,        # Reduced to avoid OOM (lower if needed)\n",
        "    'learning_rate': 0.001,  # Good for AdamW\n",
        "    'eps': 16,               # Epsilon for perturbation\n",
        "    'surrogate': 'RN50',\n",
        "    'head': 'ArcFace',\n",
        "}\n",
        "\n",
        "print(\"Training Configuration:\")\n",
        "for k, v in CONFIG.items():\n",
        "    print(f\"  {k}: {v}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Variant 1: Targeted Only\n",
        "\n",
        "This variant only uses the targeted classification loss (minimize loss for target class).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Train - Targeted Only (No Contrastive Loss)\n",
        "%cd /content/MFCLIP_acv\n",
        "\n",
        "!python main.py \\\n",
        "    --flag train_unet \\\n",
        "    --generator vit \\\n",
        "    --dataset oxford_pets \\\n",
        "    --root /content/data \\\n",
        "    --config-file configs/trainers/CoOp/rn50.yaml \\\n",
        "    --dataset-config-file configs/datasets/oxford_pets.yaml \\\n",
        "    --trainer ZeroshotCLIP \\\n",
        "    --surrogate RN50 \\\n",
        "    --head ArcFace \\\n",
        "    --loss_mode targeted_only \\\n",
        "    --num_epoch 150 \\\n",
        "    --bs 32 \\\n",
        "    --lr 0.001 \\\n",
        "    --eps 16 \\\n",
        "    --device cuda:0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Variant 2: Full Contrastive\n",
        "\n",
        "Targeted + Full contrastive loss (weight=1.0). Maximizes feature distance between clean and adversarial images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: Train - Full Contrastive (Targeted + Contrastive w/ weight=1.0)\n",
        "%cd /content/MFCLIP_acv\n",
        "\n",
        "!python main.py \\\n",
        "    --flag train_unet \\\n",
        "    --generator vit \\\n",
        "    --dataset oxford_pets \\\n",
        "    --root /content/data \\\n",
        "    --config-file configs/trainers/CoOp/rn50.yaml \\\n",
        "    --dataset-config-file configs/datasets/oxford_pets.yaml \\\n",
        "    --trainer ZeroshotCLIP \\\n",
        "    --surrogate RN50 \\\n",
        "    --head ArcFace \\\n",
        "    --loss_mode contrastive \\\n",
        "    --contrastive_weight 1.0 \\\n",
        "    --num_epoch 150 \\\n",
        "    --bs 32 \\\n",
        "    --lr 0.001 \\\n",
        "    --eps 16 \\\n",
        "    --device cuda:0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Variant 3: Mixed Loss\n",
        "\n",
        "Targeted + Weighted contrastive (weight=0.5). A balance between targeted attack and feature disruption.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: Train - Mixed (Targeted + Contrastive w/ weight=0.5)\n",
        "%cd /content/MFCLIP_acv\n",
        "\n",
        "!python main.py \\\n",
        "    --flag train_unet \\\n",
        "    --generator vit \\\n",
        "    --dataset oxford_pets \\\n",
        "    --root /content/data \\\n",
        "    --config-file configs/trainers/CoOp/rn50.yaml \\\n",
        "    --dataset-config-file configs/datasets/oxford_pets.yaml \\\n",
        "    --trainer ZeroshotCLIP \\\n",
        "    --surrogate RN50 \\\n",
        "    --head ArcFace \\\n",
        "    --loss_mode mixed \\\n",
        "    --contrastive_weight 0.5 \\\n",
        "    --num_epoch 150 \\\n",
        "    --bs 32 \\\n",
        "    --lr 0.001 \\\n",
        "    --eps 16 \\\n",
        "    --device cuda:0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Checkpoints to Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 8: Save All Checkpoints to Google Drive\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "DATA_ROOT = \"/content/data/oxford_pets\"\n",
        "DRIVE_OUTPUT = \"/content/drive/MyDrive/grad/comp_vision/hanson_loss/oxford_pets\"\n",
        "\n",
        "checkpoints = [\n",
        "    \"vit_generator_targeted_only.pt\",\n",
        "    \"vit_generator_contrastive.pt\",\n",
        "    \"vit_generator_mixed.pt\",\n",
        "    \"vit_generator_targeted_only_history.json\",\n",
        "    \"vit_generator_contrastive_history.json\",\n",
        "    \"vit_generator_mixed_history.json\",\n",
        "]\n",
        "\n",
        "print(\"Copying checkpoints to Google Drive...\")\n",
        "for ckpt in checkpoints:\n",
        "    src = f\"{DATA_ROOT}/{ckpt}\"\n",
        "    dst = f\"{DRIVE_OUTPUT}/{ckpt}\"\n",
        "    if os.path.exists(src):\n",
        "        shutil.copy(src, dst)\n",
        "        print(f\"  ✓ {ckpt}\")\n",
        "    else:\n",
        "        print(f\"  ✗ {ckpt} (not found)\")\n",
        "\n",
        "print(f\"\\nDone! Checkpoints saved to: {DRIVE_OUTPUT}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot Training Histories\n",
        "\n",
        "Compare loss and accuracy curves across the 3 variants.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 9: Plot Training Histories\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "DATA_ROOT = \"/content/data/oxford_pets\"\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "histories = {\n",
        "    'targeted_only': None,\n",
        "    'contrastive': None,\n",
        "    'mixed': None,\n",
        "}\n",
        "\n",
        "colors = {'targeted_only': 'blue', 'contrastive': 'red', 'mixed': 'green'}\n",
        "\n",
        "# Load histories\n",
        "for mode in histories.keys():\n",
        "    path = f\"{DATA_ROOT}/vit_generator_{mode}_history.json\"\n",
        "    if os.path.exists(path):\n",
        "        with open(path, 'r') as f:\n",
        "            histories[mode] = json.load(f)\n",
        "\n",
        "# Plot 1: Total Loss\n",
        "ax = axes[0]\n",
        "for mode, hist in histories.items():\n",
        "    if hist:\n",
        "        ax.plot(hist['epoch'], hist['loss'], label=mode, color=colors[mode])\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('Total Loss')\n",
        "ax.set_title('Training Loss')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Target Loss\n",
        "ax = axes[1]\n",
        "for mode, hist in histories.items():\n",
        "    if hist and 'loss_target' in hist:\n",
        "        ax.plot(hist['epoch'], hist['loss_target'], label=mode, color=colors[mode])\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('Target Loss')\n",
        "ax.set_title('Target Classification Loss')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: Accuracy\n",
        "ax = axes[2]\n",
        "for mode, hist in histories.items():\n",
        "    if hist:\n",
        "        ax.plot(hist['epoch'], hist['acc'], label=mode, color=colors[mode])\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('Accuracy')\n",
        "ax.set_title('Training Accuracy (Lower = Better Attack)')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{DATA_ROOT}/training_comparison.png\", dpi=150)\n",
        "plt.show()\n",
        "\n",
        "print(\"Plot saved to training_comparison.png\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
