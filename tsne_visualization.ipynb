{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# t-SNE visualization for CLIP features\n",
        "\n",
        "This notebook:\n",
        "1. Sets up the repo and environment (Colab-ready)\n",
        "2. Downloads Oxford Pets dataset\n",
        "3. Extracts CLIP image features and labels\n",
        "4. Runs t-SNE visualization to show discrimination ability\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Repo root: /Users/hamzaiqbal/grad/comp_vision/project_mf-clip/MFCLIP_acv\n",
            "Data root: /content/data\n",
            "Trainer cfg: /Users/hamzaiqbal/grad/comp_vision/project_mf-clip/MFCLIP_acv/configs/trainers/CoOp/rn50.yaml\n",
            "Dataset cfg: /Users/hamzaiqbal/grad/comp_vision/project_mf-clip/MFCLIP_acv/configs/datasets/oxford_pets.yaml\n",
            "Split: test\n"
          ]
        }
      ],
      "source": [
        "# 0) GPU + repo setup\n",
        "!nvidia-smi\n",
        "%cd /content\n",
        "!git clone -b hamza/discrim https://github.com/1hamzaiqbal/MFCLIP_acv\n",
        "%cd MFCLIP_acv\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running:\n",
            " /usr/local/bin/python3 /Users/hamzaiqbal/grad/comp_vision/project_mf-clip/MFCLIP_acv/lpclip/feat_extractor.py --root /content/data --output-dir /Users/hamzaiqbal/grad/comp_vision/project_mf-clip/MFCLIP_acv/clip_feat --config-file /Users/hamzaiqbal/grad/comp_vision/project_mf-clip/MFCLIP_acv/configs/trainers/CoOp/rn50.yaml --dataset-config-file /Users/hamzaiqbal/grad/comp_vision/project_mf-clip/MFCLIP_acv/configs/datasets/oxford_pets.yaml --split test\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/hamzaiqbal/grad/comp_vision/project_mf-clip/MFCLIP_acv/lpclip/feat_extractor.py\", line 3, in <module>\n",
            "    import torch\n",
            "ModuleNotFoundError: No module named 'torch'\n",
            "\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Feature extraction failed; see stderr above.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(res\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature extraction failed; see stderr above.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Expected output file path\u001b[39;00m\n\u001b[1;32m     21\u001b[0m npz_path \u001b[38;5;241m=\u001b[39m output_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOxfordPets\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.npz\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Feature extraction failed; see stderr above."
          ]
        }
      ],
      "source": [
        "# 1) Install dependencies\n",
        "!pip install torch torchvision timm einops yacs tqdm opencv-python scikit-learn scipy pyyaml ruamel.yaml pytorch-ignite foolbox pandas matplotlib seaborn wilds ftfy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'npz_path' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Load features and labels\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m npz \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[43mnpz_path\u001b[49m)\n\u001b[1;32m      8\u001b[0m X \u001b[38;5;241m=\u001b[39m npz[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_list\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m y \u001b[38;5;241m=\u001b[39m npz[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel_list\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'npz_path' is not defined"
          ]
        }
      ],
      "source": [
        "# 2) Download Oxford Pets dataset\n",
        "from torchvision.datasets import OxfordIIITPet\n",
        "from torchvision import transforms\n",
        "from pathlib import Path\n",
        "\n",
        "root = Path(\"/content/data/oxford_pets\")\n",
        "root.mkdir(parents=True, exist_ok=True)\n",
        "_ = OxfordIIITPet(root=str(root), download=True, transform=transforms.ToTensor())\n",
        "print(\"Oxford Pets downloaded to\", root)\n",
        "\n",
        "# Also fetch annotations\n",
        "%cd /content\n",
        "!mkdir -p /content/data/oxford_pets\n",
        "!wget -q https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n",
        "!wget -q https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\n",
        "!tar -xf images.tar.gz -C /content/data/oxford_pets\n",
        "!tar -xf annotations.tar.gz -C /content/data/oxford_pets\n",
        "!ls /content/data/oxford_pets/annotations | head\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3) Extract features for t-SNE visualization\n",
        "# Change split to \"train\", \"val\", or \"test\" as needed\n",
        "%cd /content/MFCLIP_acv\n",
        "%env TF_CPP_MIN_LOG_LEVEL=2\n",
        "\n",
        "split = \"test\"  # Can change to \"train\" or \"val\"\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "cmd = [\n",
        "    sys.executable, \"lpclip/feat_extractor.py\",\n",
        "    \"--root\", \"/content/data\",\n",
        "    \"--output-dir\", \"/content/MFCLIP_acv/clip_feat\",\n",
        "    \"--config-file\", \"configs/trainers/CoOp/rn50.yaml\",\n",
        "    \"--dataset-config-file\", \"configs/datasets/oxford_pets.yaml\",\n",
        "    \"--split\", split\n",
        "]\n",
        "\n",
        "print(f\"Running feature extraction for split: {split}\")\n",
        "result = subprocess.run(cmd, capture_output=True, text=True)\n",
        "print(result.stdout)\n",
        "if result.returncode != 0:\n",
        "    print(\"Error:\", result.stderr)\n",
        "    raise RuntimeError(\"Feature extraction failed\")\n",
        "\n",
        "# Verify output exists\n",
        "import os\n",
        "npz_path = f\"/content/MFCLIP_acv/clip_feat/OxfordPets/{split}.npz\"\n",
        "assert os.path.exists(npz_path), f\"Features not found at {npz_path}\"\n",
        "print(f\"\\nFeatures extracted successfully to: {npz_path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4) Run t-SNE visualization\n",
        "import numpy as np\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load features and labels\n",
        "npz = np.load(npz_path)\n",
        "X = np.array(npz[\"feature_list\"], dtype=np.float32)\n",
        "y = np.array(npz[\"label_list\"], dtype=np.int32)\n",
        "print(f\"Features shape: {X.shape}, Labels shape: {y.shape}\")\n",
        "print(f\"Number of classes: {len(np.unique(y))}\")\n",
        "\n",
        "# Optional subsample for faster computation and clearer plots\n",
        "max_points = 4000\n",
        "if len(X) > max_points:\n",
        "    rng = np.random.default_rng(0)\n",
        "    idx = rng.choice(len(X), size=max_points, replace=False)\n",
        "    X = X[idx]\n",
        "    y = y[idx]\n",
        "    print(f\"Subsampled to {len(X)} points for plotting\")\n",
        "\n",
        "# Run t-SNE\n",
        "perplexity = min(30, max(5, len(X)//100))  # Adaptive perplexity\n",
        "print(f\"Running t-SNE with perplexity={perplexity}...\")\n",
        "tsne = TSNE(n_components=2, init=\"pca\", learning_rate=\"auto\", perplexity=perplexity, random_state=0, n_iter=1000)\n",
        "Z = tsne.fit_transform(X)\n",
        "print(\"t-SNE completed!\")\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 10))\n",
        "unique_labels = np.unique(y)\n",
        "palette = sns.color_palette(\"tab20\", n_colors=len(unique_labels))\n",
        "\n",
        "for i, cls in enumerate(unique_labels):\n",
        "    mask = (y == cls)\n",
        "    plt.scatter(Z[mask, 0], Z[mask, 1], s=10, color=palette[i % len(palette)], \n",
        "                label=f\"Class {cls}\", alpha=0.6, edgecolors='none')\n",
        "\n",
        "plt.title(f\"t-SNE Visualization of CLIP Features ({split} split)\\nShowing discrimination ability across {len(unique_labels)} classes\", \n",
        "          fontsize=14, fontweight='bold')\n",
        "plt.xlabel(\"t-SNE dimension 1\", fontsize=12)\n",
        "plt.ylabel(\"t-SNE dimension 2\", fontsize=12)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8, ncol=2)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save figure\n",
        "output_path = f\"/content/MFCLIP_acv/tsne_{split}.png\"\n",
        "plt.savefig(output_path, dpi=200, bbox_inches='tight')\n",
        "print(f\"\\nSaved visualization to: {output_path}\")\n",
        "plt.show()\n",
        "\n",
        "# Print some statistics\n",
        "print(f\"\\nStatistics:\")\n",
        "print(f\"  - Total samples: {len(y)}\")\n",
        "print(f\"  - Number of classes: {len(unique_labels)}\")\n",
        "print(f\"  - Samples per class: {np.bincount(y)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
