{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1hamzaiqbal/MFCLIP_acv/blob/hamza%2Fdiscrim/vit_generator_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mount_drive"
      },
      "outputs": [],
      "source": [
        "# 1) Mount Google Drive (Run this first!)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "setup_repo"
      },
      "outputs": [],
      "source": [
        "# 2) Setup Repo & Dependencies\n",
        "!nvidia-smi\n",
        "%cd /content\n",
        "import os\n",
        "if not os.path.exists(\"MFCLIP_acv\"):\n",
        "    !git clone -b hamza/discrim https://github.com/1hamzaiqbal/MFCLIP_acv\n",
        "%cd MFCLIP_acv\n",
        "!git pull origin hamza/discrim  # Ensure latest code\n",
        "\n",
        "!pip install torch torchvision timm einops yacs tqdm opencv-python scikit-learn scipy pyyaml ruamel.yaml pytorch-ignite foolbox pandas matplotlib seaborn wilds ftfy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "setup_data"
      },
      "outputs": [],
      "source": [
        "# 3) Setup Data & Checkpoint\n",
        "import shutil\n",
        "import os\n",
        "from torchvision.datasets import OxfordIIITPet\n",
        "from torchvision import transforms\n",
        "from pathlib import Path\n",
        "\n",
        "# Download Dataset\n",
        "root = Path(\"/content/data/oxford_pets\")\n",
        "root.mkdir(parents=True, exist_ok=True)\n",
        "_ = OxfordIIITPet(root=str(root), download=True, transform=transforms.ToTensor())\n",
        "\n",
        "# Fetch Annotations\n",
        "%cd /content\n",
        "!mkdir -p /content/data/oxford_pets\n",
        "!wget -q https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n",
        "!wget -q https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\n",
        "!tar -xf images.tar.gz -C /content/data/oxford_pets\n",
        "!tar -xf annotations.tar.gz -C /content/data/oxford_pets\n",
        "\n",
        "# Copy Checkpoint from Drive (Optional if only visualizing)\n",
        "src_ckpt = \"/content/drive/MyDrive/grad/comp_vision/hanson_loss/oxford_pets/RN50_ArcFace_oxford_pets.pth\"\n",
        "dst_ckpt = \"/content/data/oxford_pets/RN50_ArcFace.pth\"\n",
        "\n",
        "if os.path.exists(src_ckpt):\n",
        "    shutil.copy(src_ckpt, dst_ckpt)\n",
        "    print(f\"Successfully copied checkpoint to {dst_ckpt}\")\n",
        "else:\n",
        "    print(f\"WARNING: Checkpoint not found at {src_ckpt}. Training will fail, but Visualization will work if you have a trained generator.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "train_vit"
      },
      "outputs": [],
      "source": [
        "# 4) Train ViT Generator (Skip this if you already have a trained model)\n",
        "%cd /content/MFCLIP_acv\n",
        "!python main.py \\\n",
        "  --flag train_unet \\\n",
        "  --generator vit \\\n",
        "  --dataset oxford_pets \\\n",
        "  --root /content/data \\\n",
        "  --config-file configs/trainers/CoOp/rn50.yaml \\\n",
        "  --dataset-config-file configs/datasets/oxford_pets.yaml \\\n",
        "  --trainer ZeroshotCLIP \\\n",
        "  --surrogate RN50 \\\n",
        "  --head ArcFace \\\n",
        "  --num_epoch 300 \\\n",
        "  --bs 64 \\\n",
        "  --lr 0.01 \\\n",
        "  --optimizer SGD \\\n",
        "  --ratio 0.2 \\\n",
        "  --device cuda:0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "save_and_plot"
      },
      "outputs": [],
      "source": [
        "# 5) Save Artifacts and Plot History\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Paths\n",
        "src_model = \"/content/data/oxford_pets/vit_generator.pt\"\n",
        "src_history = \"/content/data/oxford_pets/vit_generator_history.json\"\n",
        "dst_dir = \"/content/drive/MyDrive/grad/comp_vision/hanson_loss/oxford_pets\"\n",
        "\n",
        "# Copy to Drive\n",
        "os.makedirs(dst_dir, exist_ok=True)\n",
        "if os.path.exists(src_model):\n",
        "    shutil.copy(src_model, os.path.join(dst_dir, \"vit_generator.pt\"))\n",
        "    print(f\"Saved model to {dst_dir}/vit_generator.pt\")\n",
        "else:\n",
        "    print(\"Model file not found! (Did you skip training?)\")\n",
        "\n",
        "if os.path.exists(src_history):\n",
        "    shutil.copy(src_history, os.path.join(dst_dir, \"vit_generator_history.json\"))\n",
        "    print(f\"Saved history to {dst_dir}/vit_generator_history.json\")\n",
        "    \n",
        "    # Plot\n",
        "    with open(src_history, 'r') as f:\n",
        "        history = json.load(f)\n",
        "    \n",
        "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "    \n",
        "    color = 'tab:red'\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss', color=color)\n",
        "    ax1.plot(history['epoch'], history['loss'], color=color, label='Loss')\n",
        "    ax1.tick_params(axis='y', labelcolor=color)\n",
        "    \n",
        "    ax2 = ax1.twinx()\n",
        "    color = 'tab:blue'\n",
        "    ax2.set_ylabel('Accuracy', color=color)\n",
        "    ax2.plot(history['epoch'], history['acc'], color=color, label='Accuracy')\n",
        "    ax2.tick_params(axis='y', labelcolor=color)\n",
        "    \n",
        "    plt.title(\"ViT Generator Training Progress\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"History file not found! (Did you skip training?)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "visualize_example"
      },
      "outputs": [],
      "source": [
        "# 6) Visualize Adversarial Example\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from model import ViTGenerator\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# --- LOAD CHECKPOINT FROM DRIVE IF NEEDED ---\n",
        "local_ckpt = \"/content/data/oxford_pets/vit_generator.pt\"\n",
        "drive_ckpt = \"/content/drive/MyDrive/grad/comp_vision/hanson_loss/oxford_pets/vit_generator.pt\"\n",
        "\n",
        "if not os.path.exists(local_ckpt):\n",
        "    print(f\"Local checkpoint not found at {local_ckpt}\")\n",
        "    if os.path.exists(drive_ckpt):\n",
        "        print(f\"Found checkpoint in Drive at {drive_ckpt}. Copying...\")\n",
        "        shutil.copy(drive_ckpt, local_ckpt)\n",
        "        print(\"Copy complete.\")\n",
        "    else:\n",
        "        print(f\"WARNING: Checkpoint not found in Drive either ({drive_ckpt}). Visualization will use random weights!\")\n",
        "\n",
        "# Load Generator\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "generator = ViTGenerator().to(device)\n",
        "\n",
        "if os.path.exists(local_ckpt):\n",
        "    generator.load_state_dict(torch.load(local_ckpt, map_location=device))\n",
        "    print(\"Generator loaded successfully.\")\n",
        "else:\n",
        "    print(\"Using random weights (Generator not loaded).\")\n",
        "generator.eval()\n",
        "\n",
        "# Load a sample image\n",
        "img_path = \"/content/data/oxford_pets/images/Abyssinian_1.jpg\" # Example image\n",
        "if not os.path.exists(img_path):\n",
        "    # Fallback if specific image doesn't exist, pick first one\n",
        "    import glob\n",
        "    images = glob.glob(\"/content/data/oxford_pets/images/*.jpg\")\n",
        "    if images:\n",
        "        img_path = images[0]\n",
        "    else:\n",
        "        print(\"No images found to visualize. Did you run the 'Setup Data' cell?\")\n",
        "        img_path = None\n",
        "\n",
        "if img_path:\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "    \n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    img_t = transform(img).unsqueeze(0).to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        noise = generator(img_t)\n",
        "        # Clamp noise for visualization (eps=10/255 approx 0.04)\n",
        "        eps = 10/255\n",
        "        noise = torch.clamp(noise, -eps, eps)\n",
        "        adv_img = torch.clamp(img_t + noise, 0, 1)\n",
        "    \n",
        "    # Helper to plot\n",
        "    def show_tensor(t, ax, title):\n",
        "        im = t.squeeze().cpu().permute(1, 2, 0).numpy()\n",
        "        # Normalize noise for better visibility if needed, but here we show raw\n",
        "        if title == \"Noise (Amplified)\":\n",
        "            im = (im - im.min()) / (im.max() - im.min())\n",
        "        ax.imshow(im)\n",
        "        ax.set_title(title)\n",
        "        ax.axis('off')\n",
        "\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
        "    show_tensor(img_t, axs[0], \"Original\")\n",
        "    show_tensor(noise, axs[1], \"Noise (Amplified)\")\n",
        "    show_tensor(adv_img, axs[2], \"Adversarial\")\n",
        "    plt.show()\n"
      ]
    }
  ]
}