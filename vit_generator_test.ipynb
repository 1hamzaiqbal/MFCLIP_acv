{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1hamzaiqbal/MFCLIP_acv/blob/hamza%2Fdiscrim/vit_generator_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mount_drive"
      },
      "outputs": [],
      "source": [
        "# 1) Mount Google Drive (Run this first!)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "setup_repo"
      },
      "outputs": [],
      "source": [
        "# 2) Setup Repo & Dependencies\n",
        "!nvidia-smi\n",
        "%cd /content\n",
        "import os\n",
        "if not os.path.exists(\"MFCLIP_acv\"):\n",
        "    !git clone -b hamza/discrim https://github.com/1hamzaiqbal/MFCLIP_acv\n",
        "%cd MFCLIP_acv\n",
        "!git pull origin hamza/discrim  # Ensure latest code\n",
        "\n",
        "!pip install torch torchvision timm einops yacs tqdm opencv-python scikit-learn scipy pyyaml ruamel.yaml pytorch-ignite foolbox pandas matplotlib seaborn wilds ftfy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "setup_data"
      },
      "outputs": [],
      "source": [
        "# 3) Setup Data & Checkpoint\n",
        "import shutil\n",
        "import os\n",
        "from torchvision.datasets import OxfordIIITPet\n",
        "from torchvision import transforms\n",
        "from pathlib import Path\n",
        "\n",
        "# Download Dataset\n",
        "root = Path(\"/content/data/oxford_pets\")\n",
        "root.mkdir(parents=True, exist_ok=True)\n",
        "_ = OxfordIIITPet(root=str(root), download=True, transform=transforms.ToTensor())\n",
        "\n",
        "# Fetch Annotations\n",
        "%cd /content\n",
        "!mkdir -p /content/data/oxford_pets\n",
        "!wget -q https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n",
        "!wget -q https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\n",
        "!tar -xf images.tar.gz -C /content/data/oxford_pets\n",
        "!tar -xf annotations.tar.gz -C /content/data/oxford_pets\n",
        "\n",
        "# Copy Checkpoint from Drive (Optional if only visualizing)\n",
        "src_ckpt = \"/content/drive/MyDrive/grad/comp_vision/hanson_loss/oxford_pets/RN50_ArcFace_oxford_pets.pth\"\n",
        "dst_ckpt = \"/content/data/oxford_pets/RN50_ArcFace.pth\"\n",
        "\n",
        "if os.path.exists(src_ckpt):\n",
        "    shutil.copy(src_ckpt, dst_ckpt)\n",
        "    print(f\"Successfully copied checkpoint to {dst_ckpt}\")\n",
        "else:\n",
        "    print(f\"WARNING: Checkpoint not found at {src_ckpt}. Training will fail, but Visualization will work if you have a trained generator.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "train_vit"
      },
      "outputs": [],
      "source": [
        "# 4) Train ViT Generator (Targeted + Contrastive)\n",
        "# Note: Added --contrastive flag for Feature Disruption\n",
        "%cd /content/MFCLIP_acv\n",
        "!python main.py \\\n",
        "  --flag train_unet \\\n",
        "  --generator vit \\\n",
        "  --dataset oxford_pets \\\n",
        "  --root /content/data \\\n",
        "  --config-file configs/trainers/CoOp/rn50.yaml \\\n",
        "  --dataset-config-file configs/datasets/oxford_pets.yaml \\\n",
        "  --trainer ZeroshotCLIP \\\n",
        "  --surrogate RN50 \\\n",
        "  --head ArcFace \\\n",
        "  --num_epoch 300 \\\n",
        "  --bs 64 \\\n",
        "  --lr 0.01 \\\n",
        "  --optimizer SGD \\\n",
        "  --ratio 0.2 \\\n",
        "  --device cuda:0 \\\n",
        "  --targeted \\\n",
        "  --contrastive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "save_and_plot"
      },
      "outputs": [],
      "source": [
        "# 5) Save Artifacts and Plot History\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Paths\n",
        "src_model = \"/content/data/oxford_pets/vit_generator.pt\"\n",
        "src_history = \"/content/data/oxford_pets/vit_generator_history.json\"\n",
        "dst_dir = \"/content/drive/MyDrive/grad/comp_vision/hanson_loss/oxford_pets\"\n",
        "\n",
        "# Copy to Drive\n",
        "os.makedirs(dst_dir, exist_ok=True)\n",
        "if os.path.exists(src_model):\n",
        "    shutil.copy(src_model, os.path.join(dst_dir, \"vit_generator_contrastive.pt\"))\n",
        "    print(f\"Saved model to {dst_dir}/vit_generator_contrastive.pt\")\n",
        "else:\n",
        "    print(\"Model file not found! (Did you skip training?)\")\n",
        "\n",
        "if os.path.exists(src_history):\n",
        "    shutil.copy(src_history, os.path.join(dst_dir, \"vit_generator_contrastive_history.json\"))\n",
        "    print(f\"Saved history to {dst_dir}/vit_generator_contrastive_history.json\")\n",
        "    \n",
        "    # Plot\n",
        "    with open(src_history, 'r') as f:\n",
        "        history = json.load(f)\n",
        "    \n",
        "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "    \n",
        "    color = 'tab:red'\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss', color=color)\n",
        "    ax1.plot(history['epoch'], history['loss'], color=color, label='Loss')\n",
        "    ax1.tick_params(axis='y', labelcolor=color)\n",
        "    \n",
        "    ax2 = ax1.twinx()\n",
        "    color = 'tab:blue'\n",
        "    ax2.set_ylabel('Accuracy', color=color)\n",
        "    ax2.plot(history['epoch'], history['acc'], color=color, label='Accuracy')\n",
        "    ax2.tick_params(axis='y', labelcolor=color)\n",
        "    \n",
        "    plt.title(\"ViT Generator Training (Targeted + Contrastive)\")\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"History file not found! (Did you skip training?)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "visualize_example"
      },
      "outputs": [],
      "source": [
        "# 6) Visualize Targeted Attack & Interpolation\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from model import ViTGenerator\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "import shutil\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# --- LOAD CHECKPOINT FROM DRIVE IF NEEDED ---\n",
        "local_ckpt = \"/content/data/oxford_pets/vit_generator.pt\"\n",
        "drive_ckpt = \"/content/drive/MyDrive/grad/comp_vision/hanson_loss/oxford_pets/vit_generator_contrastive.pt\"\n",
        "\n",
        "if not os.path.exists(local_ckpt):\n",
        "    print(f\"Local checkpoint not found at {local_ckpt}\")\n",
        "    if os.path.exists(drive_ckpt):\n",
        "        print(f\"Found checkpoint in Drive at {drive_ckpt}. Copying...\")\n",
        "        shutil.copy(drive_ckpt, local_ckpt)\n",
        "        print(\"Copy complete.\")\n",
        "    else:\n",
        "        print(f\"WARNING: Checkpoint not found in Drive either ({drive_ckpt}). Visualization will use random weights!\")\n",
        "\n",
        "# Load Generator\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Note: num_classes=37 for Oxford Pets\n",
        "generator = ViTGenerator(num_classes=37).to(device)\n",
        "\n",
        "if os.path.exists(local_ckpt):\n",
        "    generator.load_state_dict(torch.load(local_ckpt, map_location=device))\n",
        "    print(\"Generator loaded successfully.\")\n",
        "else:\n",
        "    print(\"Using random weights (Generator not loaded).\")\n",
        "generator.eval()\n",
        "\n",
        "# Load a sample image\n",
        "img_path = \"/content/data/oxford_pets/images/Abyssinian_1.jpg\" # Example image\n",
        "if not os.path.exists(img_path):\n",
        "    # Fallback if specific image doesn't exist, pick first one\n",
        "    import glob\n",
        "    images = glob.glob(\"/content/data/oxford_pets/images/*.jpg\")\n",
        "    if images:\n",
        "        img_path = images[0]\n",
        "    else:\n",
        "        print(\"No images found to visualize. Did you run the 'Setup Data' cell?\")\n",
        "        img_path = None\n",
        "\n",
        "if img_path:\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "    \n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "    img_t = transform(img).unsqueeze(0).to(device)\n",
        "    \n",
        "    # --- TARGETED ATTACK ---\n",
        "    # Let's pick a target class (e.g., class 5)\n",
        "    target_class = 5\n",
        "    target_label = torch.tensor([target_class]).to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        noise = generator(img_t, target_label)\n",
        "        eps = 10/255\n",
        "        noise = torch.clamp(noise, -eps, eps)\n",
        "        adv_img = torch.clamp(img_t + noise, 0, 1)\n",
        "    \n",
        "    # --- INTERPOLATION ANALYSIS ---\n",
        "    # Interpolate between Original and Adversarial\n",
        "    alphas = np.linspace(0, 1, 11) # 0.0, 0.1, ... 1.0\n",
        "    interp_imgs = []\n",
        "    for alpha in alphas:\n",
        "        interp_img = (1 - alpha) * img_t + alpha * adv_img\n",
        "        interp_imgs.append(interp_img)\n",
        "    \n",
        "    # Helper to plot\n",
        "    def show_tensor(t, ax, title):\n",
        "        im = t.squeeze().cpu().permute(1, 2, 0).numpy()\n",
        "        if title == \"Noise (Amplified)\":\n",
        "            im = (im - im.min()) / (im.max() - im.min())\n",
        "        ax.imshow(im)\n",
        "        ax.set_title(title)\n",
        "        ax.axis('off')\n",
        "\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
        "    show_tensor(img_t, axs[0], \"Original\")\n",
        "    show_tensor(noise, axs[1], f\"Noise (Target {target_class})\")\n",
        "    show_tensor(adv_img, axs[2], \"Adversarial\")\n",
        "    plt.show()\n",
        "    \n",
        "    # Show Interpolation\n",
        "    fig, axs = plt.subplots(1, len(alphas), figsize=(20, 3))\n",
        "    for i, alpha in enumerate(alphas):\n",
        "        show_tensor(interp_imgs[i], axs[i], f\"Î±={alpha:.1f}\")\n",
        "    plt.suptitle(\"Interpolation: Original -> Adversarial\")\n",
        "    plt.show()\n"
      ]
    }
  ]
}