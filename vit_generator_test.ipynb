{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1hamzaiqbal/MFCLIP_acv/blob/hamza%2Fdiscrim/vit_generator_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzfIg5NpoHNm"
      },
      "outputs": [],
      "source": [
        "# 0) GPU + repo\n",
        "!nvidia-smi\n",
        "%cd /content\n",
        "!git clone -b hamza/discrim https://github.com/1hamzaiqbal/MFCLIP_acv\n",
        "%cd MFCLIP_acv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Deps\n",
        "!pip install torch torchvision timm einops yacs tqdm opencv-python scikit-learn scipy pyyaml ruamel.yaml pytorch-ignite foolbox pandas matplotlib seaborn wilds ftfy timm\n",
        "\n"
      ],
      "metadata": {
        "id": "BDljgHXio74B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Data (example: Oxford Pets into /content/data/oxford_pets)\n",
        "from torchvision.datasets import OxfordIIITPet\n",
        "from torchvision import transforms\n",
        "from pathlib import Path\n",
        "root = Path(\"/content/data/oxford_pets\")\n",
        "root.mkdir(parents=True, exist_ok=True)\n",
        "_ = OxfordIIITPet(root=str(root), download=True, transform=transforms.ToTensor())\n",
        "print(\"Oxford Pets downloaded to\", root)\n",
        "\n"
      ],
      "metadata": {
        "id": "CfYAXPuMo_WM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fetch official Oxford-IIIT Pet dataset (provides annotations/trainval.txt)\n",
        "%cd /content\n",
        "!mkdir -p /content/data/oxford_pets\n",
        "!wget -q https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n",
        "!wget -q https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\n",
        "!tar -xf images.tar.gz -C /content/data/oxford_pets\n",
        "!tar -xf annotations.tar.gz -C /content/data/oxford_pets\n",
        "!ls /content/data/oxford_pets/annotations | head\n"
      ],
      "metadata": {
        "id": "_gQQc7Lpug3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) Mount Drive and Setup Checkpoint\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define paths\n",
        "src_ckpt = \"/content/drive/MyDrive/grad/comp_vision/hanson_loss/oxford_pets/RN50_ArcFace_oxford_pets.pth\"\n",
        "dst_dir = \"/content/data/oxford_pets\"\n",
        "dst_ckpt = os.path.join(dst_dir, \"RN50_ArcFace.pth\")\n",
        "\n",
        "# Copy checkpoint\n",
        "os.makedirs(dst_dir, exist_ok=True)\n",
        "if os.path.exists(src_ckpt):\n",
        "    shutil.copy(src_ckpt, dst_ckpt)\n",
        "    print(f\"Copied {src_ckpt} to {dst_ckpt}\")\n",
        "else:\n",
        "    print(f\"Warning: Checkpoint not found at {src_ckpt}. Please ensure Drive is mounted and path is correct.\")\n"
      ],
      "metadata": {
        "id": "9Z8EzZ8HpFC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) Train ViT Generator\n",
        "!python main.py \\\n",
        "  --flag train_unet \\\n",
        "  --generator vit \\\n",
        "  --dataset oxford_pets \\\n",
        "  --root /content/data \\\n",
        "  --config-file configs/trainers/CoOp/rn50.yaml \\\n",
        "  --dataset-config-file configs/datasets/oxford_pets.yaml \\\n",
        "  --trainer ZeroshotCLIP \\\n",
        "  --surrogate RN50 \\\n",
        "  --head ArcFace \\\n",
        "  --num_epoch 1 \\\n",
        "  --bs 64 \\\n",
        "  --lr 0.01 \\\n",
        "  --optimizer SGD \\\n",
        "  --ratio 0.2 \\\n",
        "  --device cuda:0\n"
      ],
      "metadata": {
        "id": "8L4JipAfqEIB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "c0bc62d0-009f-493f-87da-ca9be4268d0e"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MFCLIP_acv\n",
            "2025-11-30 05:07:41.967409: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1764479261.988662   12400 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1764479261.995082   12400 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1764479262.011586   12400 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764479262.011619   12400 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764479262.011622   12400 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1764479262.011625   12400 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "Loading trainer: ZeroshotCLIP\n",
            "Loading dataset: OxfordPets\n",
            "Reading split from /content/data/oxford_pets/split_zhou_OxfordPets.json\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0, 0, 0], std=[1, 1, 1])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0, 0, 0], std=[1, 1, 1])\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "---------  ----------\n",
            "Dataset    OxfordPets\n",
            "# classes  37\n",
            "# train_x  2,944\n",
            "# val      736\n",
            "# test     3,669\n",
            "---------  ----------\n",
            "Loading dataset: OxfordPets\n",
            "Reading split from /content/data/oxford_pets/split_zhou_OxfordPets.json\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0, 0, 0], std=[1, 1, 1])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0, 0, 0], std=[1, 1, 1])\n",
            "---------  ----------\n",
            "Dataset    OxfordPets\n",
            "# classes  37\n",
            "# train_x  2,944\n",
            "# val      736\n",
            "# test     3,669\n",
            "---------  ----------\n",
            "Loading dataset: OxfordPets\n",
            "Reading split from /content/data/oxford_pets/split_zhou_OxfordPets.json\n",
            "Building transform_train\n",
            "+ random resized crop (size=(224, 224), scale=(0.08, 1.0))\n",
            "+ random flip\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0, 0, 0], std=[1, 1, 1])\n",
            "Building transform_test\n",
            "+ resize the smaller edge to 224\n",
            "+ 224x224 center crop\n",
            "+ to torch tensor of range [0, 1]\n",
            "+ normalization (mean=[0, 0, 0], std=[1, 1, 1])\n",
            "---------  ----------\n",
            "Dataset    OxfordPets\n",
            "# classes  37\n",
            "# train_x  2,944\n",
            "# val      736\n",
            "# test     3,669\n",
            "---------  ----------\n",
            "Loading CLIP (backbone: RN50)\n",
            "Prompts: ['a photo of a abyssinian, a type of pet.', 'a photo of a american bulldog, a type of pet.', 'a photo of a american pit bull terrier, a type of pet.', 'a photo of a basset hound, a type of pet.', 'a photo of a beagle, a type of pet.', 'a photo of a bengal, a type of pet.', 'a photo of a birman, a type of pet.', 'a photo of a bombay, a type of pet.', 'a photo of a boxer, a type of pet.', 'a photo of a british shorthair, a type of pet.', 'a photo of a chihuahua, a type of pet.', 'a photo of a egyptian mau, a type of pet.', 'a photo of a english cocker spaniel, a type of pet.', 'a photo of a english setter, a type of pet.', 'a photo of a german shorthaired, a type of pet.', 'a photo of a great pyrenees, a type of pet.', 'a photo of a havanese, a type of pet.', 'a photo of a japanese chin, a type of pet.', 'a photo of a keeshond, a type of pet.', 'a photo of a leonberger, a type of pet.', 'a photo of a maine coon, a type of pet.', 'a photo of a miniature pinscher, a type of pet.', 'a photo of a newfoundland, a type of pet.', 'a photo of a persian, a type of pet.', 'a photo of a pomeranian, a type of pet.', 'a photo of a pug, a type of pet.', 'a photo of a ragdoll, a type of pet.', 'a photo of a russian blue, a type of pet.', 'a photo of a saint bernard, a type of pet.', 'a photo of a samoyed, a type of pet.', 'a photo of a scottish terrier, a type of pet.', 'a photo of a shiba inu, a type of pet.', 'a photo of a siamese, a type of pet.', 'a photo of a sphynx, a type of pet.', 'a photo of a staffordshire bull terrier, a type of pet.', 'a photo of a wheaten terrier, a type of pet.', 'a photo of a yorkshire terrier, a type of pet.']\n",
            "Loading evaluator: Classification\n",
            "dataset loaded\n",
            "head param:\n",
            "{'feat_dim': 1024, 'num_class': 37, 'margin_arc': 0.15, 'margin_am': 0.0, 'scale': 16}\n",
            "setup surrogate head: ArcFace\n",
            "model architecture: \n",
            "\n",
            " Model(\n",
            "  (backbone): Sequential(\n",
            "    (0): Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
            "    (1): ModifiedResNet(\n",
            "      (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu2): ReLU(inplace=True)\n",
            "      (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu3): ReLU(inplace=True)\n",
            "      (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "      (layer1): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu1): ReLU(inplace=True)\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu2): ReLU(inplace=True)\n",
            "          (avgpool): Identity()\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu3): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (-1): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
            "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu1): ReLU(inplace=True)\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu2): ReLU(inplace=True)\n",
            "          (avgpool): Identity()\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu3): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu1): ReLU(inplace=True)\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu2): ReLU(inplace=True)\n",
            "          (avgpool): Identity()\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu3): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (layer2): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu1): ReLU(inplace=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu2): ReLU(inplace=True)\n",
            "          (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu3): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu1): ReLU(inplace=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu2): ReLU(inplace=True)\n",
            "          (avgpool): Identity()\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu3): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu1): ReLU(inplace=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu2): ReLU(inplace=True)\n",
            "          (avgpool): Identity()\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu3): ReLU(inplace=True)\n",
            "        )\n",
            "        (3): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu1): ReLU(inplace=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu2): ReLU(inplace=True)\n",
            "          (avgpool): Identity()\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu3): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (layer3): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu1): ReLU(inplace=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu2): ReLU(inplace=True)\n",
            "          (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu3): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu1): ReLU(inplace=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu2): ReLU(inplace=True)\n",
            "          (avgpool): Identity()\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu3): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu1): ReLU(inplace=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu2): ReLU(inplace=True)\n",
            "          (avgpool): Identity()\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu3): ReLU(inplace=True)\n",
            "        )\n",
            "        (3): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu1): ReLU(inplace=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu2): ReLU(inplace=True)\n",
            "          (avgpool): Identity()\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu3): ReLU(inplace=True)\n",
            "        )\n",
            "        (4): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu1): ReLU(inplace=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu2): ReLU(inplace=True)\n",
            "          (avgpool): Identity()\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu3): ReLU(inplace=True)\n",
            "        )\n",
            "        (5): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu1): ReLU(inplace=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu2): ReLU(inplace=True)\n",
            "          (avgpool): Identity()\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu3): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (layer4): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu1): ReLU(inplace=True)\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu2): ReLU(inplace=True)\n",
            "          (avgpool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu3): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (-1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu1): ReLU(inplace=True)\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu2): ReLU(inplace=True)\n",
            "          (avgpool): Identity()\n",
            "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu3): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu1): ReLU(inplace=True)\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu2): ReLU(inplace=True)\n",
            "          (avgpool): Identity()\n",
            "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu3): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (attnpool): AttentionPool2d(\n",
            "        (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
            "        (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
            "        (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
            "        (c_proj): Linear(in_features=2048, out_features=1024, bias=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (head): ArcFace()\n",
            ") \n",
            "\n",
            "\n",
            "Trainable parameters:\n",
            "backbone.1.conv1.weight | shape: (32, 3, 3, 3)\n",
            "backbone.1.bn1.weight | shape: (32,)\n",
            "backbone.1.bn1.bias | shape: (32,)\n",
            "backbone.1.conv2.weight | shape: (32, 32, 3, 3)\n",
            "backbone.1.bn2.weight | shape: (32,)\n",
            "backbone.1.bn2.bias | shape: (32,)\n",
            "backbone.1.conv3.weight | shape: (64, 32, 3, 3)\n",
            "backbone.1.bn3.weight | shape: (64,)\n",
            "backbone.1.bn3.bias | shape: (64,)\n",
            "backbone.1.layer1.0.conv1.weight | shape: (64, 64, 1, 1)\n",
            "backbone.1.layer1.0.bn1.weight | shape: (64,)\n",
            "backbone.1.layer1.0.bn1.bias | shape: (64,)\n",
            "backbone.1.layer1.0.conv2.weight | shape: (64, 64, 3, 3)\n",
            "backbone.1.layer1.0.bn2.weight | shape: (64,)\n",
            "backbone.1.layer1.0.bn2.bias | shape: (64,)\n",
            "backbone.1.layer1.0.conv3.weight | shape: (256, 64, 1, 1)\n",
            "backbone.1.layer1.0.bn3.weight | shape: (256,)\n",
            "backbone.1.layer1.0.bn3.bias | shape: (256,)\n",
            "backbone.1.layer1.0.downsample.0.weight | shape: (256, 64, 1, 1)\n",
            "backbone.1.layer1.0.downsample.1.weight | shape: (256,)\n",
            "backbone.1.layer1.0.downsample.1.bias | shape: (256,)\n",
            "backbone.1.layer1.1.conv1.weight | shape: (64, 256, 1, 1)\n",
            "backbone.1.layer1.1.bn1.weight | shape: (64,)\n",
            "backbone.1.layer1.1.bn1.bias | shape: (64,)\n",
            "backbone.1.layer1.1.conv2.weight | shape: (64, 64, 3, 3)\n",
            "backbone.1.layer1.1.bn2.weight | shape: (64,)\n",
            "backbone.1.layer1.1.bn2.bias | shape: (64,)\n",
            "backbone.1.layer1.1.conv3.weight | shape: (256, 64, 1, 1)\n",
            "backbone.1.layer1.1.bn3.weight | shape: (256,)\n",
            "backbone.1.layer1.1.bn3.bias | shape: (256,)\n",
            "backbone.1.layer1.2.conv1.weight | shape: (64, 256, 1, 1)\n",
            "backbone.1.layer1.2.bn1.weight | shape: (64,)\n",
            "backbone.1.layer1.2.bn1.bias | shape: (64,)\n",
            "backbone.1.layer1.2.conv2.weight | shape: (64, 64, 3, 3)\n",
            "backbone.1.layer1.2.bn2.weight | shape: (64,)\n",
            "backbone.1.layer1.2.bn2.bias | shape: (64,)\n",
            "backbone.1.layer1.2.conv3.weight | shape: (256, 64, 1, 1)\n",
            "backbone.1.layer1.2.bn3.weight | shape: (256,)\n",
            "backbone.1.layer1.2.bn3.bias | shape: (256,)\n",
            "backbone.1.layer2.0.conv1.weight | shape: (128, 256, 1, 1)\n",
            "backbone.1.layer2.0.bn1.weight | shape: (128,)\n",
            "backbone.1.layer2.0.bn1.bias | shape: (128,)\n",
            "backbone.1.layer2.0.conv2.weight | shape: (128, 128, 3, 3)\n",
            "backbone.1.layer2.0.bn2.weight | shape: (128,)\n",
            "backbone.1.layer2.0.bn2.bias | shape: (128,)\n",
            "backbone.1.layer2.0.conv3.weight | shape: (512, 128, 1, 1)\n",
            "backbone.1.layer2.0.bn3.weight | shape: (512,)\n",
            "backbone.1.layer2.0.bn3.bias | shape: (512,)\n",
            "backbone.1.layer2.0.downsample.0.weight | shape: (512, 256, 1, 1)\n",
            "backbone.1.layer2.0.downsample.1.weight | shape: (512,)\n",
            "backbone.1.layer2.0.downsample.1.bias | shape: (512,)\n",
            "backbone.1.layer2.1.conv1.weight | shape: (128, 512, 1, 1)\n",
            "backbone.1.layer2.1.bn1.weight | shape: (128,)\n",
            "backbone.1.layer2.1.bn1.bias | shape: (128,)\n",
            "backbone.1.layer2.1.conv2.weight | shape: (128, 128, 3, 3)\n",
            "backbone.1.layer2.1.bn2.weight | shape: (128,)\n",
            "backbone.1.layer2.1.bn2.bias | shape: (128,)\n",
            "backbone.1.layer2.1.conv3.weight | shape: (512, 128, 1, 1)\n",
            "backbone.1.layer2.1.bn3.weight | shape: (512,)\n",
            "backbone.1.layer2.1.bn3.bias | shape: (512,)\n",
            "backbone.1.layer2.2.conv1.weight | shape: (128, 512, 1, 1)\n",
            "backbone.1.layer2.2.bn1.weight | shape: (128,)\n",
            "backbone.1.layer2.2.bn1.bias | shape: (128,)\n",
            "backbone.1.layer2.2.conv2.weight | shape: (128, 128, 3, 3)\n",
            "backbone.1.layer2.2.bn2.weight | shape: (128,)\n",
            "backbone.1.layer2.2.bn2.bias | shape: (128,)\n",
            "backbone.1.layer2.2.conv3.weight | shape: (512, 128, 1, 1)\n",
            "backbone.1.layer2.2.bn3.weight | shape: (512,)\n",
            "backbone.1.layer2.2.bn3.bias | shape: (512,)\n",
            "backbone.1.layer2.3.conv1.weight | shape: (128, 512, 1, 1)\n",
            "backbone.1.layer2.3.bn1.weight | shape: (128,)\n",
            "backbone.1.layer2.3.bn1.bias | shape: (128,)\n",
            "backbone.1.layer2.3.conv2.weight | shape: (128, 128, 3, 3)\n",
            "backbone.1.layer2.3.bn2.weight | shape: (128,)\n",
            "backbone.1.layer2.3.bn2.bias | shape: (128,)\n",
            "backbone.1.layer2.3.conv3.weight | shape: (512, 128, 1, 1)\n",
            "backbone.1.layer2.3.bn3.weight | shape: (512,)\n",
            "backbone.1.layer2.3.bn3.bias | shape: (512,)\n",
            "backbone.1.layer3.0.conv1.weight | shape: (256, 512, 1, 1)\n",
            "backbone.1.layer3.0.bn1.weight | shape: (256,)\n",
            "backbone.1.layer3.0.bn1.bias | shape: (256,)\n",
            "backbone.1.layer3.0.conv2.weight | shape: (256, 256, 3, 3)\n",
            "backbone.1.layer3.0.bn2.weight | shape: (256,)\n",
            "backbone.1.layer3.0.bn2.bias | shape: (256,)\n",
            "backbone.1.layer3.0.conv3.weight | shape: (1024, 256, 1, 1)\n",
            "backbone.1.layer3.0.bn3.weight | shape: (1024,)\n",
            "backbone.1.layer3.0.bn3.bias | shape: (1024,)\n",
            "backbone.1.layer3.0.downsample.0.weight | shape: (1024, 512, 1, 1)\n",
            "backbone.1.layer3.0.downsample.1.weight | shape: (1024,)\n",
            "backbone.1.layer3.0.downsample.1.bias | shape: (1024,)\n",
            "backbone.1.layer3.1.conv1.weight | shape: (256, 1024, 1, 1)\n",
            "backbone.1.layer3.1.bn1.weight | shape: (256,)\n",
            "backbone.1.layer3.1.bn1.bias | shape: (256,)\n",
            "backbone.1.layer3.1.conv2.weight | shape: (256, 256, 3, 3)\n",
            "backbone.1.layer3.1.bn2.weight | shape: (256,)\n",
            "backbone.1.layer3.1.bn2.bias | shape: (256,)\n",
            "backbone.1.layer3.1.conv3.weight | shape: (1024, 256, 1, 1)\n",
            "backbone.1.layer3.1.bn3.weight | shape: (1024,)\n",
            "backbone.1.layer3.1.bn3.bias | shape: (1024,)\n",
            "backbone.1.layer3.2.conv1.weight | shape: (256, 1024, 1, 1)\n",
            "backbone.1.layer3.2.bn1.weight | shape: (256,)\n",
            "backbone.1.layer3.2.bn1.bias | shape: (256,)\n",
            "backbone.1.layer3.2.conv2.weight | shape: (256, 256, 3, 3)\n",
            "backbone.1.layer3.2.bn2.weight | shape: (256,)\n",
            "backbone.1.layer3.2.bn2.bias | shape: (256,)\n",
            "backbone.1.layer3.2.conv3.weight | shape: (1024, 256, 1, 1)\n",
            "backbone.1.layer3.2.bn3.weight | shape: (1024,)\n",
            "backbone.1.layer3.2.bn3.bias | shape: (1024,)\n",
            "backbone.1.layer3.3.conv1.weight | shape: (256, 1024, 1, 1)\n",
            "backbone.1.layer3.3.bn1.weight | shape: (256,)\n",
            "backbone.1.layer3.3.bn1.bias | shape: (256,)\n",
            "backbone.1.layer3.3.conv2.weight | shape: (256, 256, 3, 3)\n",
            "backbone.1.layer3.3.bn2.weight | shape: (256,)\n",
            "backbone.1.layer3.3.bn2.bias | shape: (256,)\n",
            "backbone.1.layer3.3.conv3.weight | shape: (1024, 256, 1, 1)\n",
            "backbone.1.layer3.3.bn3.weight | shape: (1024,)\n",
            "backbone.1.layer3.3.bn3.bias | shape: (1024,)\n",
            "backbone.1.layer3.4.conv1.weight | shape: (256, 1024, 1, 1)\n",
            "backbone.1.layer3.4.bn1.weight | shape: (256,)\n",
            "backbone.1.layer3.4.bn1.bias | shape: (256,)\n",
            "backbone.1.layer3.4.conv2.weight | shape: (256, 256, 3, 3)\n",
            "backbone.1.layer3.4.bn2.weight | shape: (256,)\n",
            "backbone.1.layer3.4.bn2.bias | shape: (256,)\n",
            "backbone.1.layer3.4.conv3.weight | shape: (1024, 256, 1, 1)\n",
            "backbone.1.layer3.4.bn3.weight | shape: (1024,)\n",
            "backbone.1.layer3.4.bn3.bias | shape: (1024,)\n",
            "backbone.1.layer3.5.conv1.weight | shape: (256, 1024, 1, 1)\n",
            "backbone.1.layer3.5.bn1.weight | shape: (256,)\n",
            "backbone.1.layer3.5.bn1.bias | shape: (256,)\n",
            "backbone.1.layer3.5.conv2.weight | shape: (256, 256, 3, 3)\n",
            "backbone.1.layer3.5.bn2.weight | shape: (256,)\n",
            "backbone.1.layer3.5.bn2.bias | shape: (256,)\n",
            "backbone.1.layer3.5.conv3.weight | shape: (1024, 256, 1, 1)\n",
            "backbone.1.layer3.5.bn3.weight | shape: (1024,)\n",
            "backbone.1.layer3.5.bn3.bias | shape: (1024,)\n",
            "backbone.1.layer4.0.conv1.weight | shape: (512, 1024, 1, 1)\n",
            "backbone.1.layer4.0.bn1.weight | shape: (512,)\n",
            "backbone.1.layer4.0.bn1.bias | shape: (512,)\n",
            "backbone.1.layer4.0.conv2.weight | shape: (512, 512, 3, 3)\n",
            "backbone.1.layer4.0.bn2.weight | shape: (512,)\n",
            "backbone.1.layer4.0.bn2.bias | shape: (512,)\n",
            "backbone.1.layer4.0.conv3.weight | shape: (2048, 512, 1, 1)\n",
            "backbone.1.layer4.0.bn3.weight | shape: (2048,)\n",
            "backbone.1.layer4.0.bn3.bias | shape: (2048,)\n",
            "backbone.1.layer4.0.downsample.0.weight | shape: (2048, 1024, 1, 1)\n",
            "backbone.1.layer4.0.downsample.1.weight | shape: (2048,)\n",
            "backbone.1.layer4.0.downsample.1.bias | shape: (2048,)\n",
            "backbone.1.layer4.1.conv1.weight | shape: (512, 2048, 1, 1)\n",
            "backbone.1.layer4.1.bn1.weight | shape: (512,)\n",
            "backbone.1.layer4.1.bn1.bias | shape: (512,)\n",
            "backbone.1.layer4.1.conv2.weight | shape: (512, 512, 3, 3)\n",
            "backbone.1.layer4.1.bn2.weight | shape: (512,)\n",
            "backbone.1.layer4.1.bn2.bias | shape: (512,)\n",
            "backbone.1.layer4.1.conv3.weight | shape: (2048, 512, 1, 1)\n",
            "backbone.1.layer4.1.bn3.weight | shape: (2048,)\n",
            "backbone.1.layer4.1.bn3.bias | shape: (2048,)\n",
            "backbone.1.layer4.2.conv1.weight | shape: (512, 2048, 1, 1)\n",
            "backbone.1.layer4.2.bn1.weight | shape: (512,)\n",
            "backbone.1.layer4.2.bn1.bias | shape: (512,)\n",
            "backbone.1.layer4.2.conv2.weight | shape: (512, 512, 3, 3)\n",
            "backbone.1.layer4.2.bn2.weight | shape: (512,)\n",
            "backbone.1.layer4.2.bn2.bias | shape: (512,)\n",
            "backbone.1.layer4.2.conv3.weight | shape: (2048, 512, 1, 1)\n",
            "backbone.1.layer4.2.bn3.weight | shape: (2048,)\n",
            "backbone.1.layer4.2.bn3.bias | shape: (2048,)\n",
            "backbone.1.attnpool.positional_embedding | shape: (50, 2048)\n",
            "backbone.1.attnpool.k_proj.weight | shape: (2048, 2048)\n",
            "backbone.1.attnpool.k_proj.bias | shape: (2048,)\n",
            "backbone.1.attnpool.q_proj.weight | shape: (2048, 2048)\n",
            "backbone.1.attnpool.q_proj.bias | shape: (2048,)\n",
            "backbone.1.attnpool.v_proj.weight | shape: (2048, 2048)\n",
            "backbone.1.attnpool.v_proj.bias | shape: (2048,)\n",
            "backbone.1.attnpool.c_proj.weight | shape: (1024, 2048)\n",
            "backbone.1.attnpool.c_proj.bias | shape: (1024,)\n",
            "head.weight | shape: (1024, 37)\n",
            "Epoch: 0, train acc: 0.3893, loss: 5.908616\n",
            "Epoch: 1, train acc: 0.1349, loss: 3.354581\n",
            "Epoch: 2, train acc: 0.0641, loss: 1.004577\n",
            "Epoch: 3, train acc: 0.0414, loss: -2.406761\n",
            "Epoch: 4, train acc: 0.0359, loss: -4.418786\n",
            "Epoch: 5, train acc: 0.0307, loss: -5.054022\n",
            "Epoch: 6, train acc: 0.0296, loss: -5.276598\n",
            "Epoch: 7, train acc: 0.0282, loss: -5.456719\n",
            "Epoch: 8, train acc: 0.0296, loss: -5.604998\n",
            "Epoch: 9, train acc: 0.0285, loss: -5.709706\n",
            "Epoch: 10, train acc: 0.0302, loss: -5.773343\n",
            "Epoch: 11, train acc: 0.0280, loss: -5.905611\n",
            "Epoch: 12, train acc: 0.0285, loss: -5.968924\n",
            "Epoch: 13, train acc: 0.0280, loss: -6.034438\n",
            "Epoch: 14, train acc: 0.0282, loss: -6.118891\n",
            "Epoch: 15, train acc: 0.0285, loss: -6.174362\n",
            "Epoch: 16, train acc: 0.0271, loss: -6.274500\n",
            "Epoch: 17, train acc: 0.0282, loss: -6.302677\n",
            "Epoch: 18, train acc: 0.0280, loss: -6.321428\n",
            "Epoch: 19, train acc: 0.0280, loss: -6.368742\n",
            "Epoch: 20, train acc: 0.0277, loss: -6.480219\n",
            "Epoch: 21, train acc: 0.0277, loss: -6.482142\n",
            "Epoch: 22, train acc: 0.0271, loss: -6.551657\n",
            "Epoch: 23, train acc: 0.0277, loss: -6.559633\n",
            "Epoch: 24, train acc: 0.0271, loss: -6.597878\n",
            "Epoch: 25, train acc: 0.0277, loss: -6.626320\n",
            "Epoch: 26, train acc: 0.0277, loss: -6.677326\n",
            "Epoch: 27, train acc: 0.0271, loss: -6.735778\n",
            "Epoch: 28, train acc: 0.0277, loss: -6.757090\n",
            "Epoch: 29, train acc: 0.0277, loss: -6.752361\n",
            "Epoch: 30, train acc: 0.0274, loss: -6.788307\n",
            "Epoch: 31, train acc: 0.0277, loss: -6.833206\n",
            "Epoch: 32, train acc: 0.0277, loss: -6.857850\n",
            "Epoch: 33, train acc: 0.0269, loss: -6.873912\n",
            "Epoch: 34, train acc: 0.0266, loss: -6.894517\n",
            "Epoch: 35, train acc: 0.0274, loss: -6.957748\n",
            "Epoch: 36, train acc: 0.0271, loss: -6.966775\n",
            "Epoch: 37, train acc: 0.0274, loss: -7.000355\n",
            "Epoch: 38, train acc: 0.0274, loss: -7.008095\n",
            "Epoch: 39, train acc: 0.0271, loss: -7.069087\n",
            "Epoch: 40, train acc: 0.0274, loss: -7.080423\n",
            "Epoch: 41, train acc: 0.0274, loss: -7.085009\n",
            "Epoch: 42, train acc: 0.0271, loss: -7.137464\n",
            "Epoch: 43, train acc: 0.0271, loss: -7.149798\n",
            "Epoch: 44, train acc: 0.0271, loss: -7.163565\n",
            "Epoch: 45, train acc: 0.0271, loss: -7.212893\n",
            "Epoch: 46, train acc: 0.0271, loss: -7.224943\n",
            "Epoch: 47, train acc: 0.0269, loss: -7.257774\n",
            "Epoch: 48, train acc: 0.0271, loss: -7.275011\n",
            "Epoch: 49, train acc: 0.0271, loss: -7.285787\n",
            "Epoch: 50, train acc: 0.0269, loss: -7.304396\n",
            "Epoch: 51, train acc: 0.0271, loss: -7.298406\n",
            "Epoch: 52, train acc: 0.0274, loss: -7.347689\n",
            "Epoch: 53, train acc: 0.0274, loss: -7.347799\n",
            "Epoch: 54, train acc: 0.0269, loss: -7.357614\n",
            "Epoch: 55, train acc: 0.0274, loss: -7.368204\n",
            "Epoch: 56, train acc: 0.0269, loss: -7.399949\n",
            "Epoch: 57, train acc: 0.0271, loss: -7.410074\n",
            "Epoch: 58, train acc: 0.0271, loss: -7.453705\n",
            "Epoch: 59, train acc: 0.0271, loss: -7.459446\n",
            "Epoch: 60, train acc: 0.0271, loss: -7.489700\n",
            "Epoch: 61, train acc: 0.0274, loss: -7.498129\n",
            "Epoch: 62, train acc: 0.0269, loss: -7.518241\n",
            "Epoch: 63, train acc: 0.0271, loss: -7.524052\n",
            "Epoch: 64, train acc: 0.0274, loss: -7.534762\n",
            "Epoch: 65, train acc: 0.0271, loss: -7.553715\n",
            "Epoch: 66, train acc: 0.0271, loss: -7.579888\n",
            "Epoch: 67, train acc: 0.0274, loss: -7.587285\n",
            "Epoch: 68, train acc: 0.0271, loss: -7.598194\n",
            "Epoch: 69, train acc: 0.0269, loss: -7.636134\n",
            "Epoch: 70, train acc: 0.0271, loss: -7.648470\n",
            "Epoch: 71, train acc: 0.0274, loss: -7.625151\n",
            "Epoch: 72, train acc: 0.0269, loss: -7.641893\n",
            "Epoch: 73, train acc: 0.0269, loss: -7.701998\n",
            "Epoch: 74, train acc: 0.0271, loss: -7.705764\n",
            "Epoch: 75, train acc: 0.0266, loss: -7.702538\n",
            "Epoch: 76, train acc: 0.0271, loss: -7.716332\n",
            "Epoch: 77, train acc: 0.0269, loss: -7.741457\n",
            "Epoch: 78, train acc: 0.0271, loss: -7.753717\n",
            "Epoch: 79, train acc: 0.0271, loss: -7.770843\n",
            "Epoch: 80, train acc: 0.0271, loss: -7.753781\n",
            "Epoch: 81, train acc: 0.0269, loss: -7.779660\n",
            "Epoch: 82, train acc: 0.0271, loss: -7.764036\n",
            "Epoch: 83, train acc: 0.0271, loss: -7.781695\n",
            "Epoch: 84, train acc: 0.0269, loss: -7.813591\n",
            "Epoch: 85, train acc: 0.0269, loss: -7.822616\n",
            "Epoch: 86, train acc: 0.0271, loss: -7.843428\n",
            "Epoch: 87, train acc: 0.0249, loss: -7.812328\n",
            "Epoch: 88, train acc: 0.0126, loss: -7.838494\n",
            "Epoch: 89, train acc: 0.0082, loss: -7.956119\n",
            "Epoch: 90, train acc: 0.0077, loss: -7.987417\n",
            "Epoch: 91, train acc: 0.0063, loss: -7.937398\n",
            "Epoch: 92, train acc: 0.0044, loss: -8.011110\n",
            "Epoch: 93, train acc: 0.0052, loss: -8.040380\n",
            "Epoch: 94, train acc: 0.0047, loss: -8.056406\n",
            "Epoch: 95, train acc: 0.0041, loss: -8.113518\n",
            "Epoch: 96, train acc: 0.0038, loss: -8.035971\n",
            "Epoch: 97, train acc: 0.0038, loss: -8.155692\n",
            "Epoch: 98, train acc: 0.0041, loss: -8.159624\n",
            "Epoch: 99, train acc: 0.0027, loss: -8.176083\n",
            "Epoch: 100, train acc: 0.0038, loss: -8.206505\n",
            "Epoch: 101, train acc: 0.0041, loss: -8.218781\n",
            "Epoch: 102, train acc: 0.0033, loss: -8.247582\n",
            "Epoch: 103, train acc: 0.0025, loss: -8.289392\n",
            "Epoch: 104, train acc: 0.0027, loss: -8.285256\n",
            "Epoch: 105, train acc: 0.0044, loss: -8.247677\n",
            "Epoch: 106, train acc: 0.0019, loss: -8.319590\n",
            "Epoch: 107, train acc: 0.0033, loss: -8.299048\n",
            "Epoch: 108, train acc: 0.0038, loss: -8.303974\n",
            "Epoch: 109, train acc: 0.0027, loss: -8.322757\n",
            "Epoch: 110, train acc: 0.0016, loss: -8.349543\n",
            "Epoch: 111, train acc: 0.0036, loss: -8.322502\n",
            "Epoch: 112, train acc: 0.0011, loss: -8.357419\n",
            "Epoch: 113, train acc: 0.0027, loss: -8.354605\n",
            "Epoch: 114, train acc: 0.0027, loss: -8.344258\n",
            "Epoch: 115, train acc: 0.0022, loss: -8.379801\n",
            "Epoch: 116, train acc: 0.0022, loss: -8.370091\n",
            "Epoch: 117, train acc: 0.0014, loss: -8.365935\n",
            "Epoch: 118, train acc: 0.0014, loss: -8.396845\n",
            "Epoch: 119, train acc: 0.0022, loss: -8.391612\n",
            "Epoch: 120, train acc: 0.0030, loss: -8.377770\n",
            "Epoch: 121, train acc: 0.0014, loss: -8.418062\n",
            "Epoch: 122, train acc: 0.0022, loss: -8.437123\n",
            "Epoch: 123, train acc: 0.0019, loss: -8.415597\n",
            "Epoch: 124, train acc: 0.0025, loss: -8.402814\n",
            "Epoch: 125, train acc: 0.0022, loss: -8.432405\n",
            "Epoch: 126, train acc: 0.0011, loss: -8.438156\n",
            "Epoch: 127, train acc: 0.0027, loss: -8.455812\n",
            "Epoch: 128, train acc: 0.0025, loss: -8.427454\n",
            "Epoch: 129, train acc: 0.0016, loss: -8.451209\n",
            "Epoch: 130, train acc: 0.0019, loss: -8.457404\n",
            "Epoch: 131, train acc: 0.0016, loss: -8.481968\n",
            "Epoch: 132, train acc: 0.0008, loss: -8.454382\n",
            "Epoch: 133, train acc: 0.0016, loss: -8.456779\n",
            "Epoch: 134, train acc: 0.0016, loss: -8.473554\n",
            "Epoch: 135, train acc: 0.0019, loss: -8.467395\n",
            "Epoch: 136, train acc: 0.0014, loss: -8.465334\n",
            "Epoch: 137, train acc: 0.0008, loss: -8.490784\n",
            "Epoch: 138, train acc: 0.0027, loss: -8.457759\n",
            "Epoch: 139, train acc: 0.0014, loss: -8.478978\n",
            "Epoch: 140, train acc: 0.0016, loss: -8.445852\n",
            "Epoch: 141, train acc: 0.0011, loss: -8.475293\n",
            "Epoch: 142, train acc: 0.0011, loss: -8.477920\n",
            "Epoch: 143, train acc: 0.0008, loss: -8.483651\n",
            "Epoch: 144, train acc: 0.0011, loss: -8.492262\n",
            "Epoch: 145, train acc: 0.0022, loss: -8.454039\n",
            "Epoch: 146, train acc: 0.0019, loss: -8.466448\n",
            "Epoch: 147, train acc: 0.0014, loss: -8.470770\n",
            "Epoch: 148, train acc: 0.0014, loss: -8.466398\n",
            "Epoch: 149, train acc: 0.0030, loss: -8.460301\n",
            "Epoch: 150, train acc: 0.0055, loss: -8.053018\n",
            "Epoch: 151, train acc: 0.0079, loss: -7.906761\n",
            "Epoch: 152, train acc: 0.0044, loss: -7.903556\n",
            "Epoch: 153, train acc: 0.0049, loss: -7.949242\n",
            "Epoch: 154, train acc: 0.0041, loss: -7.855848\n",
            "Epoch: 155, train acc: 0.0049, loss: -7.882347\n",
            "Epoch: 156, train acc: 0.0055, loss: -7.825459\n",
            "Epoch: 157, train acc: 0.0049, loss: -7.827123\n",
            "Epoch: 158, train acc: 0.0038, loss: -7.963033\n",
            "Epoch: 159, train acc: 0.0033, loss: -8.042167\n",
            "Epoch: 160, train acc: 0.0044, loss: -7.915751\n",
            "Epoch: 161, train acc: 0.0052, loss: -7.959951\n",
            "Epoch: 162, train acc: 0.0052, loss: -7.934969\n",
            "Epoch: 163, train acc: 0.0058, loss: -7.874830\n",
            "Epoch: 164, train acc: 0.0011, loss: -7.921267\n",
            "Epoch: 165, train acc: 0.0022, loss: -8.099108\n",
            "Epoch: 166, train acc: 0.0027, loss: -7.992857\n",
            "Epoch: 167, train acc: 0.0030, loss: -7.964576\n",
            "Epoch: 168, train acc: 0.0030, loss: -7.955196\n",
            "Epoch: 169, train acc: 0.0038, loss: -7.684633\n",
            "Epoch: 170, train acc: 0.0049, loss: -7.947381\n",
            "Epoch: 171, train acc: 0.0022, loss: -8.104212\n",
            "Epoch: 172, train acc: 0.0033, loss: -8.052837\n",
            "Epoch: 173, train acc: 0.0088, loss: -7.883619\n",
            "Epoch: 174, train acc: 0.0033, loss: -8.055849\n",
            "Epoch: 175, train acc: 0.0025, loss: -8.154216\n",
            "Epoch: 176, train acc: 0.0038, loss: -8.001501\n",
            "Epoch: 177, train acc: 0.0030, loss: -8.015456\n",
            "Epoch: 178, train acc: 0.0027, loss: -8.040501\n",
            "Epoch: 179, train acc: 0.0016, loss: -8.024829\n",
            "Epoch: 180, train acc: 0.0022, loss: -8.187323\n",
            "Epoch: 181, train acc: 0.0044, loss: -8.110501\n",
            "Epoch: 182, train acc: 0.0044, loss: -7.967625\n",
            "Epoch: 183, train acc: 0.0047, loss: -8.085733\n",
            "Epoch: 184, train acc: 0.0060, loss: -8.076750\n",
            "Epoch: 185, train acc: 0.0014, loss: -8.205884\n",
            "Epoch: 186, train acc: 0.0019, loss: -8.189281\n",
            "Epoch: 187, train acc: 0.0025, loss: -8.169051\n",
            "Epoch: 188, train acc: 0.0011, loss: -8.232175\n",
            "Epoch: 189, train acc: 0.0022, loss: -8.345478\n",
            "Epoch: 190, train acc: 0.0027, loss: -8.264975\n",
            "Epoch: 191, train acc: 0.0030, loss: -8.116604\n",
            "Epoch: 192, train acc: 0.0033, loss: -7.924865\n",
            "Epoch: 193, train acc: 0.0019, loss: -8.092234\n",
            "Epoch: 194, train acc: 0.0044, loss: -8.042344\n",
            "Epoch: 195, train acc: 0.0033, loss: -8.105702\n",
            "Epoch: 196, train acc: 0.0044, loss: -8.083128\n",
            "Epoch: 197, train acc: 0.0033, loss: -8.261280\n",
            "Epoch: 198, train acc: 0.0019, loss: -8.348648\n",
            "Epoch: 199, train acc: 0.0049, loss: -8.287075\n",
            "Epoch: 200, train acc: 0.0019, loss: -8.357465\n",
            "Epoch: 201, train acc: 0.0025, loss: -8.451105\n",
            "Epoch: 202, train acc: 0.0082, loss: -8.220890\n",
            "Epoch: 203, train acc: 0.0038, loss: -8.382425\n",
            "Epoch: 204, train acc: 0.0016, loss: -8.416080\n",
            "Epoch: 205, train acc: 0.0025, loss: -8.452477\n",
            "Epoch: 206, train acc: 0.0022, loss: -8.526575\n",
            "Epoch: 207, train acc: 0.0027, loss: -8.504333\n",
            "Epoch: 208, train acc: 0.0016, loss: -8.403243\n",
            "Epoch: 209, train acc: 0.0016, loss: -8.378165\n",
            "Epoch: 210, train acc: 0.0008, loss: -8.459722\n",
            "Epoch: 211, train acc: 0.0008, loss: -8.429343\n",
            "Epoch: 212, train acc: 0.0036, loss: -8.369009\n",
            "Epoch: 213, train acc: 0.0033, loss: -8.420289\n",
            "Epoch: 214, train acc: 0.0016, loss: -8.525370\n",
            "Epoch: 215, train acc: 0.0011, loss: -8.476096\n",
            "Epoch: 216, train acc: 0.0025, loss: -8.448888\n",
            "Epoch: 217, train acc: 0.0019, loss: -8.551814\n",
            "Epoch: 218, train acc: 0.0030, loss: -8.531777\n",
            "Epoch: 219, train acc: 0.0008, loss: -8.608517\n",
            "Epoch: 220, train acc: 0.0022, loss: -8.647182\n",
            "Epoch: 221, train acc: 0.0025, loss: -8.594560\n",
            "Epoch: 222, train acc: 0.0011, loss: -8.677957\n",
            "Epoch: 223, train acc: 0.0019, loss: -8.627964\n",
            "Epoch: 224, train acc: 0.0019, loss: -8.574197\n",
            "Epoch: 225, train acc: 0.0027, loss: -8.642253\n",
            "Epoch: 226, train acc: 0.0005, loss: -8.737343\n",
            "Epoch: 227, train acc: 0.0008, loss: -8.636789\n",
            "Epoch: 228, train acc: 0.0036, loss: -8.364918\n",
            "Epoch: 229, train acc: 0.0030, loss: -8.587587\n",
            "Epoch: 230, train acc: 0.0030, loss: -8.598180\n",
            "Epoch: 231, train acc: 0.0022, loss: -8.663151\n",
            "Epoch: 232, train acc: 0.0003, loss: -8.719819\n",
            "Epoch: 233, train acc: 0.0014, loss: -8.691227\n",
            "Epoch: 234, train acc: 0.0016, loss: -8.670273\n",
            "Epoch: 235, train acc: 0.0011, loss: -8.675843\n",
            "Epoch: 236, train acc: 0.0016, loss: -8.677956\n",
            "Epoch: 237, train acc: 0.0011, loss: -8.705345\n",
            "Epoch: 238, train acc: 0.0011, loss: -8.750170\n",
            "Epoch: 239, train acc: 0.0011, loss: -8.717617\n",
            "Epoch: 240, train acc: 0.0019, loss: -8.704939\n",
            "Epoch: 241, train acc: 0.0011, loss: -8.685066\n",
            "Epoch: 242, train acc: 0.0003, loss: -8.724305\n",
            "Epoch: 243, train acc: 0.0011, loss: -8.760666\n",
            "Epoch: 244, train acc: 0.0011, loss: -8.795692\n",
            "Epoch: 245, train acc: 0.0008, loss: -8.808320\n",
            "Epoch: 246, train acc: 0.0022, loss: -8.754939\n",
            "Epoch: 247, train acc: 0.0022, loss: -8.790386\n",
            "Epoch: 248, train acc: 0.0008, loss: -8.715595\n",
            "Epoch: 249, train acc: 0.0011, loss: -8.798930\n",
            "Epoch: 250, train acc: 0.0008, loss: -8.853944\n",
            "Epoch: 251, train acc: 0.0019, loss: -8.862808\n",
            "Epoch: 252, train acc: 0.0011, loss: -8.868412\n",
            "Epoch: 253, train acc: 0.0011, loss: -8.883423\n",
            "Epoch: 254, train acc: 0.0008, loss: -8.895579\n",
            "Epoch: 255, train acc: 0.0014, loss: -8.894406\n",
            "Epoch: 256, train acc: 0.0016, loss: -8.872193\n",
            "Epoch: 257, train acc: 0.0011, loss: -8.879230\n",
            "Epoch: 258, train acc: 0.0005, loss: -8.899296\n",
            "Epoch: 259, train acc: 0.0022, loss: -8.835481\n",
            "Epoch: 260, train acc: 0.0016, loss: -8.836377\n",
            "Epoch: 261, train acc: 0.0016, loss: -8.861521\n",
            "Epoch: 262, train acc: 0.0008, loss: -8.889241\n",
            "Epoch: 263, train acc: 0.0003, loss: -8.892892\n",
            "Epoch: 264, train acc: 0.0011, loss: -8.912742\n",
            "Epoch: 265, train acc: 0.0016, loss: -8.910578\n",
            "Epoch: 266, train acc: 0.0003, loss: -8.914316\n",
            "Epoch: 267, train acc: 0.0014, loss: -8.939049\n",
            "Epoch: 268, train acc: 0.0014, loss: -8.905784\n",
            "Epoch: 269, train acc: 0.0014, loss: -8.910518\n",
            "Epoch: 270, train acc: 0.0016, loss: -8.938942\n",
            "Epoch: 271, train acc: 0.0005, loss: -8.972460\n",
            "Epoch: 272, train acc: 0.0003, loss: -8.972624\n",
            "Epoch: 273, train acc: 0.0005, loss: -8.960189\n",
            "Epoch: 274, train acc: 0.0008, loss: -8.935194\n",
            "Epoch: 275, train acc: 0.0014, loss: -8.939787\n",
            "Epoch: 276, train acc: 0.0014, loss: -8.957778\n",
            "Epoch: 277, train acc: 0.0011, loss: -8.985753\n",
            "Epoch: 278, train acc: 0.0022, loss: -8.937566\n",
            "Epoch: 279, train acc: 0.0011, loss: -8.976110\n",
            "Epoch: 280, train acc: 0.0008, loss: -8.976534\n",
            "Epoch: 281, train acc: 0.0019, loss: -8.969375\n",
            "Epoch: 282, train acc: 0.0014, loss: -8.971809\n",
            "Epoch: 283, train acc: 0.0011, loss: -9.001399\n",
            "Epoch: 284, train acc: 0.0005, loss: -8.984980\n",
            "Epoch: 285, train acc: 0.0011, loss: -8.989827\n",
            "Epoch: 286, train acc: 0.0011, loss: -8.964560\n",
            "Epoch: 287, train acc: 0.0011, loss: -8.988603\n",
            "Epoch: 288, train acc: 0.0027, loss: -8.947273\n",
            "Epoch: 289, train acc: 0.0014, loss: -8.992022\n",
            "Epoch: 290, train acc: 0.0016, loss: -8.973463\n",
            "Epoch: 291, train acc: 0.0008, loss: -8.973606\n",
            "Epoch: 292, train acc: 0.0008, loss: -9.031733\n",
            "Epoch: 293, train acc: 0.0011, loss: -9.000514\n",
            "Epoch: 294, train acc: 0.0011, loss: -8.999352\n",
            "Epoch: 295, train acc: 0.0008, loss: -8.975043\n",
            "Epoch: 296, train acc: 0.0016, loss: -8.972130\n",
            "Epoch: 297, train acc: 0.0019, loss: -8.967939\n",
            "Epoch: 298, train acc: 0.0008, loss: -8.981858\n",
            "Epoch: 299, train acc: 0.0005, loss: -9.011511\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5) Save Artifacts and Plot History\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Paths\n",
        "src_model = \"/content/data/oxford_pets/vit_generator.pt\"\n",
        "src_history = \"/content/data/oxford_pets/vit_generator_history.json\"\n",
        "dst_dir = \"/content/drive/MyDrive/grad/comp_vision/hanson_loss/oxford_pets\"\n",
        "\n",
        "# Copy to Drive\n",
        "os.makedirs(dst_dir, exist_ok=True)\n",
        "if os.path.exists(src_model):\n",
        "    shutil.copy(src_model, os.path.join(dst_dir, \"vit_generator.pt\"))\n",
        "    print(f\"Saved model to {dst_dir}/vit_generator.pt\")\n",
        "else:\n",
        "    print(\"Model file not found!\")\n",
        "\n",
        "if os.path.exists(src_history):\n",
        "    shutil.copy(src_history, os.path.join(dst_dir, \"vit_generator_history.json\"))\n",
        "    print(f\"Saved history to {dst_dir}/vit_generator_history.json\")\n",
        "\n",
        "    # Plot\n",
        "    with open(src_history, 'r') as f:\n",
        "        history = json.load(f)\n",
        "\n",
        "    fig, ax1 = plt.subplots()\n",
        "\n",
        "    color = 'tab:red'\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss', color=color)\n",
        "    ax1.plot(history['epoch'], history['loss'], color=color)\n",
        "    ax1.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "    ax2 = ax1.twinx()\n",
        "    color = 'tab:blue'\n",
        "    ax2.set_ylabel('Accuracy', color=color)\n",
        "    ax2.plot(history['epoch'], history['acc'], color=color)\n",
        "    ax2.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "    plt.title(\"Training Progress\")\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"History file not found!\")\n"
      ],
      "metadata": {
        "id": "plot_and_save",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "0e4b0d1e-ccdd-44a8-c864-dc0a388e4cd0"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved model to /content/drive/MyDrive/grad/comp_vision/hanson_loss/oxford_pets/vit_generator.pt\n",
            "History file not found!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "4LiFlPjP_nNI",
        "outputId": "10019f56-75b2-47b0-9b96-fc94a99c029e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K1zhWiWhLzfY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}