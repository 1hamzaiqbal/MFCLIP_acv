{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# t-SNE Visualization: CLIP Features with Different Loss Functions\n",
        "\n",
        "This notebook visualizes how different **loss functions** affect the learned CLIP feature space.\n",
        "\n",
        "**What we're comparing:**\n",
        "- Vanilla CLIP features (no finetuning)\n",
        "- ArcFace-trained features\n",
        "- SigLip-trained features\n",
        "- ArcFaceSigmoid-trained features\n",
        "\n",
        "**Key Question:** Do different loss functions produce better class separation?\n",
        "\n",
        "**Expected Result:** ArcFace and similar metric learning losses should produce tighter, more separated clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: GPU + Repo Setup\n",
        "!nvidia-smi\n",
        "%cd /content\n",
        "\n",
        "import os\n",
        "if not os.path.exists(\"MFCLIP_acv\"):\n",
        "    !git clone -b hamza/discrim https://github.com/1hamzaiqbal/MFCLIP_acv\n",
        "\n",
        "%cd MFCLIP_acv\n",
        "!git fetch --all\n",
        "!git reset --hard origin/hamza/discrim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: Install Dependencies\n",
        "!pip install -q torch torchvision timm einops yacs tqdm opencv-python scikit-learn scipy pyyaml ruamel.yaml pytorch-ignite foolbox pandas matplotlib seaborn wilds ftfy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Download Oxford Pets Dataset\n",
        "from torchvision.datasets import OxfordIIITPet\n",
        "from torchvision import transforms\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "root = Path(\"/content/data/oxford_pets\")\n",
        "root.mkdir(parents=True, exist_ok=True)\n",
        "_ = OxfordIIITPet(root=str(root), download=True, transform=transforms.ToTensor())\n",
        "print(\"Oxford Pets downloaded\")\n",
        "\n",
        "%cd /content\n",
        "if not os.path.exists(\"/content/data/oxford_pets/images\"):\n",
        "    print(\"Downloading images and annotations...\")\n",
        "    !wget -q https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n",
        "    !wget -q https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\n",
        "    !tar -xf images.tar.gz -C /content/data/oxford_pets\n",
        "    !tar -xf annotations.tar.gz -C /content/data/oxford_pets\n",
        "    !rm -f images.tar.gz annotations.tar.gz\n",
        "print(\"Dataset ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "DRIVE_ROOT = \"/content/drive/MyDrive/grad/comp_vision/hanson_loss/oxford_pets\"\n",
        "\n",
        "print(\"\\nAvailable checkpoints in Drive:\")\n",
        "if os.path.exists(DRIVE_ROOT):\n",
        "    for f in sorted(os.listdir(DRIVE_ROOT)):\n",
        "        if f.endswith('.pth') or f.endswith('.pt'):\n",
        "            size_mb = os.path.getsize(f\"{DRIVE_ROOT}/{f}\") / 1e6\n",
        "            print(f\"  {f} ({size_mb:.1f} MB)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Define Checkpoint Paths - EDIT THESE AS NEEDED\n",
        "CHECKPOINTS = {\n",
        "    \"ArcFace\": f\"{DRIVE_ROOT}/RN50_ArcFace_oxford_pets.pth\",\n",
        "    \"ArcFaceSigmoid\": f\"{DRIVE_ROOT}/RN50_ArcFaceSigmoid_300ep.pth\",\n",
        "    \"SigLip\": f\"{DRIVE_ROOT}/RN50_SigLipHead_300ep.pth\",\n",
        "}\n",
        "\n",
        "print(\"Checkpoint availability:\")\n",
        "available_checkpoints = {}\n",
        "for name, path in CHECKPOINTS.items():\n",
        "    if os.path.exists(path):\n",
        "        print(f\"  OK {name}\")\n",
        "        available_checkpoints[name] = path\n",
        "    else:\n",
        "        print(f\"  MISSING {name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: Setup Python Path\n",
        "import sys\n",
        "sys.path.insert(0, \"/content/MFCLIP_acv/lpclip\")\n",
        "sys.path.insert(0, \"/content/MFCLIP_acv\")\n",
        "os.makedirs(\"/content/MFCLIP_acv/lpclip/datasets\", exist_ok=True)\n",
        "open(\"/content/MFCLIP_acv/lpclip/datasets/__init__.py\", \"a\").close()\n",
        "print(\"Python path configured\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: Extract Vanilla CLIP Features\n",
        "%cd /content/MFCLIP_acv\n",
        "\n",
        "DATA = \"/content/data\"\n",
        "FEAT_DIR = \"/content/MFCLIP_acv/clip_feat\"\n",
        "\n",
        "print(\"Extracting vanilla CLIP features...\")\n",
        "!python -m lpclip.feat_extractor --split test --root \"{DATA}\" --seed 1 --dataset-config-file configs/datasets/oxford_pets.yaml --config-file configs/trainers/CoOp/rn50_val.yaml --output-dir \"{FEAT_DIR}\" --eval-only\n",
        "\n",
        "print(\"\\nFeature extraction complete!\")\n",
        "!ls -l {FEAT_DIR}/OxfordPets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 8: Define t-SNE Helper Functions\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics import silhouette_score\n",
        "from scipy.spatial.distance import pdist\n",
        "\n",
        "FEAT_DIR = \"/content/MFCLIP_acv/clip_feat/OxfordPets\"\n",
        "OUT_DIR = \"/content/MFCLIP_acv/tsne_results\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "def load_npz(split=\"test\"):\n",
        "    d = np.load(os.path.join(FEAT_DIR, f\"{split}.npz\"))\n",
        "    return d[\"feature_list\"].astype(np.float32), d[\"label_list\"].astype(np.int64)\n",
        "\n",
        "def torch_load_any(path):\n",
        "    sd = torch.load(path, map_location=\"cpu\")\n",
        "    return sd[\"state_dict\"] if isinstance(sd, dict) and \"state_dict\" in sd else sd\n",
        "\n",
        "def guess_head_weight(sd, in_dim):\n",
        "    cands = [(k,v) for k,v in sd.items() if isinstance(v, torch.Tensor) and v.ndim==2]\n",
        "    for k,v in cands:\n",
        "        if v.shape[0] == in_dim:\n",
        "            return v.numpy(), k\n",
        "    return None, None\n",
        "\n",
        "def cosine_project(X, W):\n",
        "    Xn = X / np.maximum(np.linalg.norm(X, axis=1, keepdims=True), 1e-12)\n",
        "    Wn = W / np.maximum(np.linalg.norm(W, axis=0, keepdims=True), 1e-12)\n",
        "    return Xn @ Wn\n",
        "\n",
        "def run_single_tsne(X, y, title, perplexity=30):\n",
        "    print(f\"Running t-SNE for: {title}...\")\n",
        "    Z = TSNE(n_components=2, perplexity=perplexity, random_state=42, init=\"pca\", n_iter=1000).fit_transform(X)\n",
        "    try:\n",
        "        sil = silhouette_score(Z, y)\n",
        "    except:\n",
        "        sil = np.nan\n",
        "    return Z, sil\n",
        "\n",
        "print(\"Helper functions defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 9: Run t-SNE on All Variants\n",
        "X_vanilla, y = load_npz(\"test\")\n",
        "print(f\"Loaded {len(X_vanilla)} samples with {len(np.unique(y))} classes\")\n",
        "print(f\"Feature dim: {X_vanilla.shape[1]}\")\n",
        "\n",
        "all_results = {}\n",
        "\n",
        "# Vanilla CLIP\n",
        "Z_vanilla, sil_vanilla = run_single_tsne(X_vanilla, y, \"Vanilla CLIP\")\n",
        "all_results[\"Vanilla CLIP\"] = {\"embeddings\": Z_vanilla, \"silhouette\": sil_vanilla, \"features\": X_vanilla}\n",
        "\n",
        "# Process each checkpoint\n",
        "for name, path in available_checkpoints.items():\n",
        "    print(f\"\\n--- Processing {name} ---\")\n",
        "    sd = torch_load_any(path)\n",
        "    W, key = guess_head_weight(sd, X_vanilla.shape[1])\n",
        "    \n",
        "    if W is not None:\n",
        "        print(f\"  Using weight: {key} {W.shape}\")\n",
        "        X_proj = cosine_project(X_vanilla, W)\n",
        "        Z, sil = run_single_tsne(X_proj, y, name)\n",
        "        all_results[name] = {\"embeddings\": Z, \"silhouette\": sil, \"features\": X_proj}\n",
        "    else:\n",
        "        print(f\"  Could not find projection weights\")\n",
        "\n",
        "print(f\"\\nProcessed {len(all_results)} variants\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 10: Visualize All Variants Side-by-Side\n",
        "n_variants = len(all_results)\n",
        "cols = min(n_variants, 3)\n",
        "rows = (n_variants + cols - 1) // cols\n",
        "\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(6*cols, 5*rows), dpi=150)\n",
        "if n_variants == 1:\n",
        "    axes = np.array([axes])\n",
        "axes = axes.flatten()\n",
        "\n",
        "cmap = plt.cm.get_cmap('tab20', len(np.unique(y)))\n",
        "\n",
        "for idx, (name, data) in enumerate(all_results.items()):\n",
        "    ax = axes[idx]\n",
        "    Z = data[\"embeddings\"]\n",
        "    sil = data[\"silhouette\"]\n",
        "    scatter = ax.scatter(Z[:, 0], Z[:, 1], c=y, s=10, alpha=0.7, cmap=cmap)\n",
        "    ax.set_title(f\"{name}\\n(Silhouette: {sil:.3f})\", fontsize=11)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "\n",
        "for idx in range(n_variants, len(axes)):\n",
        "    axes[idx].axis('off')\n",
        "\n",
        "plt.suptitle(\"t-SNE: CLIP Features with Different Loss Functions (Oxford Pets)\", fontsize=14, y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{OUT_DIR}/tsne_loss_comparison.png\", dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f\"\\nSaved to {OUT_DIR}/tsne_loss_comparison.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 11: Quantitative Metrics Summary\n",
        "import pandas as pd\n",
        "\n",
        "def compute_distance_metrics(X, y):\n",
        "    classes = np.unique(y)\n",
        "    intra_dists = []\n",
        "    for c in classes:\n",
        "        mask = (y == c)\n",
        "        if mask.sum() > 1:\n",
        "            X_c = X[mask]\n",
        "            dists = pdist(X_c, 'cosine')\n",
        "            if len(dists) > 0:\n",
        "                intra_dists.append(np.mean(dists))\n",
        "    centroids = np.array([X[y == c].mean(axis=0) for c in classes])\n",
        "    inter_dists = pdist(centroids, 'cosine')\n",
        "    return np.mean(intra_dists) if intra_dists else np.nan, np.mean(inter_dists)\n",
        "\n",
        "results_table = []\n",
        "for name, data in all_results.items():\n",
        "    intra, inter = compute_distance_metrics(data[\"features\"], y)\n",
        "    results_table.append({\n",
        "        \"Loss Function\": name,\n",
        "        \"Silhouette Score\": data[\"silhouette\"],\n",
        "        \"Intra-class Dist\": intra,\n",
        "        \"Inter-class Dist\": inter,\n",
        "        \"Ratio (inter/intra)\": inter / intra if intra > 0 else np.nan\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(results_table)\n",
        "df = df.sort_values(\"Silhouette Score\", ascending=False)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"CLIP Feature Quality Metrics (by Loss Function)\")\n",
        "print(\"=\" * 70)\n",
        "print(df.to_string(index=False))\n",
        "print(\"=\" * 70)\n",
        "print(\"\\nHigher Silhouette Score = better cluster separation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 12: Plot Metrics Bar Chart\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "names = df[\"Loss Function\"].tolist()\n",
        "colors = plt.cm.Set2(np.linspace(0, 1, len(names)))\n",
        "\n",
        "ax = axes[0]\n",
        "bars = ax.barh(names, df[\"Silhouette Score\"], color=colors)\n",
        "ax.set_xlabel(\"Silhouette Score (higher = better)\")\n",
        "ax.set_title(\"Cluster Quality\")\n",
        "ax.invert_yaxis()\n",
        "\n",
        "ax = axes[1]\n",
        "bars = ax.barh(names, df[\"Ratio (inter/intra)\"], color=colors)\n",
        "ax.set_xlabel(\"Inter/Intra Distance Ratio (higher = better)\")\n",
        "ax.set_title(\"Class Separation\")\n",
        "ax.invert_yaxis()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(f\"{OUT_DIR}/metrics_comparison.png\", dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 13: Save Results to Google Drive\n",
        "import shutil\n",
        "\n",
        "SAVE_DIR = f\"{DRIVE_ROOT}/tsne_clip_features\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "files_to_save = [\n",
        "    f\"{OUT_DIR}/tsne_loss_comparison.png\",\n",
        "    f\"{OUT_DIR}/metrics_comparison.png\",\n",
        "]\n",
        "\n",
        "print(f\"Saving results to {SAVE_DIR}...\")\n",
        "for f in files_to_save:\n",
        "    if os.path.exists(f):\n",
        "        shutil.copy(f, SAVE_DIR)\n",
        "        print(f\"  OK {os.path.basename(f)}\")\n",
        "\n",
        "df.to_csv(f\"{SAVE_DIR}/clip_feature_metrics.csv\", index=False)\n",
        "print(f\"  OK clip_feature_metrics.csv\")\n",
        "print(f\"\\nAll results saved to {SAVE_DIR}\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

