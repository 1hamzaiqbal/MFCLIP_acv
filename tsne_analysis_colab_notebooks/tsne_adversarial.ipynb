{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# t-SNE Visualization: Clean vs Adversarial Features\n",
        "\n",
        "This notebook visualizes how different generator variants affect the CLIP feature space.\n",
        "\n",
        "**Generators to Compare:**\n",
        "- UNet Vanilla (existing)\n",
        "- UNet Contrastive (existing)\n",
        "- ViT Targeted Only (after training)\n",
        "- ViT Contrastive (after training)\n",
        "- ViT Mixed (after training)\n",
        "\n",
        "**Key Question:** Does contrastive loss training produce perturbations that disperse features MORE than targeted-only training?\n",
        "\n",
        "**Expected Result:** Contrastive-trained generators should push adversarial features further from clean feature clusters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: Mount Drive & Setup\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "import os\n",
        "DRIVE_ROOT = \"/content/drive/MyDrive/grad/comp_vision/hanson_loss/oxford_pets\"\n",
        "print(f\"Drive root: {DRIVE_ROOT}\")\n",
        "\n",
        "# List available checkpoints\n",
        "print(\"\\nAvailable checkpoints in Drive:\")\n",
        "if os.path.exists(DRIVE_ROOT):\n",
        "    for f in os.listdir(DRIVE_ROOT):\n",
        "        size_mb = os.path.getsize(f\"{DRIVE_ROOT}/{f}\") / 1e6\n",
        "        print(f\"  {f} ({size_mb:.1f} MB)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: Clone Repo & Install Dependencies\n",
        "!nvidia-smi\n",
        "%cd /content\n",
        "\n",
        "import os\n",
        "if not os.path.exists(\"MFCLIP_acv\"):\n",
        "    !git clone -b hamza/discrim https://github.com/1hamzaiqbal/MFCLIP_acv\n",
        "\n",
        "%cd MFCLIP_acv\n",
        "!git fetch --all\n",
        "!git reset --hard origin/hamza/discrim\n",
        "\n",
        "!pip install -q torch torchvision timm einops yacs tqdm opencv-python \\\n",
        "    scikit-learn scipy pyyaml ruamel.yaml pytorch-ignite foolbox \\\n",
        "    pandas matplotlib seaborn wilds ftfy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Setup Dataset\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from torchvision.datasets import OxfordIIITPet\n",
        "from torchvision import transforms\n",
        "\n",
        "DATA_ROOT = \"/content/data\"\n",
        "PETS_ROOT = f\"{DATA_ROOT}/oxford_pets\"\n",
        "DRIVE_ROOT = \"/content/drive/MyDrive/grad/comp_vision/hanson_loss/oxford_pets\"\n",
        "\n",
        "Path(PETS_ROOT).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Download via torchvision (creates split files)\n",
        "print(\"Setting up Oxford Pets...\")\n",
        "_ = OxfordIIITPet(root=PETS_ROOT, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "# Fetch images/annotations\n",
        "%cd /content\n",
        "if not os.path.exists(f\"{PETS_ROOT}/images\"):\n",
        "    !wget -q https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n",
        "    !wget -q https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\n",
        "    !tar -xf images.tar.gz -C {PETS_ROOT}\n",
        "    !tar -xf annotations.tar.gz -C {PETS_ROOT}\n",
        "    !rm -f images.tar.gz annotations.tar.gz\n",
        "print(\"✓ Dataset ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Load Surrogate Model (CLIP Backbone)\n",
        "%cd /content/MFCLIP_acv\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "from ruamel.yaml import YAML\n",
        "import sys\n",
        "sys.path.insert(0, '/content/MFCLIP_acv')\n",
        "\n",
        "from model import UNetLikeGenerator as UNet, ViTGenerator\n",
        "from utils.util import setup_cfg, Model\n",
        "from dass.engine import build_trainer\n",
        "from loss.head.head_def import HeadFactory\n",
        "\n",
        "# Register modules\n",
        "import trainers.zsclip, trainers.coop, trainers.cocoop\n",
        "import datasets.oxford_pets, datasets.oxford_flowers, datasets.food101\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Build trainer to get CLIP backbone\n",
        "class Args:\n",
        "    root = \"/content/data\"\n",
        "    dataset = \"oxford_pets\"\n",
        "    config_file = \"configs/trainers/CoOp/rn50.yaml\"\n",
        "    dataset_config_file = \"configs/datasets/oxford_pets.yaml\"\n",
        "    trainer = \"ZeroshotCLIP\"\n",
        "    head = \"ArcFace\"\n",
        "    output_dir = \"output\"\n",
        "    opts = []\n",
        "    gpu = 0\n",
        "    device = \"cuda:0\"\n",
        "    resume = \"\"\n",
        "    seed = -1\n",
        "    source_domains = None\n",
        "    target_domains = None\n",
        "    transforms = None\n",
        "    backbone = \"\"\n",
        "    bs = 64\n",
        "    ratio = 1.0\n",
        "\n",
        "args = Args()\n",
        "cfg = setup_cfg(args)\n",
        "trainer = build_trainer(cfg)\n",
        "\n",
        "# Build surrogate model\n",
        "yaml_parser = YAML(typ='safe')\n",
        "config = yaml_parser.load(open('configs/data.yaml', 'r'))\n",
        "config['num_classes'] = trainer.dm.num_classes\n",
        "config['output_dim'] = 1024\n",
        "head_factory = HeadFactory(args.head, config)\n",
        "\n",
        "clip_backbone = trainer.clip_model.visual\n",
        "normalize = transforms.Normalize([0.48145466, 0.4578275, 0.40821073], [0.26862954, 0.26130258, 0.27577711])\n",
        "backbone = nn.Sequential(normalize, clip_backbone)\n",
        "\n",
        "surrogate = Model(backbone, head_factory).to(device)\n",
        "\n",
        "# Load surrogate weights (exact filename)\n",
        "SURROGATE_PATH = f\"{DRIVE_ROOT}/RN50_ArcFace_oxford_pets.pth\"\n",
        "if os.path.exists(SURROGATE_PATH):\n",
        "    surrogate.load_state_dict(torch.load(SURROGATE_PATH, map_location=device))\n",
        "    print(f\"✓ Surrogate loaded from {SURROGATE_PATH}\")\n",
        "else:\n",
        "    print(f\"⚠️ Surrogate not found at {SURROGATE_PATH}\")\n",
        "        \n",
        "surrogate.eval()\n",
        "test_loader = trainer.test_loader\n",
        "NUM_CLASSES = trainer.dm.num_classes\n",
        "print(f\"✓ Test set: {len(test_loader.dataset)} samples, {NUM_CLASSES} classes\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Feature Extraction Functions\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "def extract_features(loader, backbone, generator=None, num_samples=1000, eps=16/255., device='cuda'):\n",
        "    \"\"\"\n",
        "    Extract features from images, optionally with adversarial perturbations.\n",
        "    \n",
        "    Args:\n",
        "        loader: DataLoader\n",
        "        backbone: Feature extractor (CLIP visual)\n",
        "        generator: Optional generator for adversarial perturbations\n",
        "        num_samples: Max samples to extract (for speed)\n",
        "        eps: Perturbation epsilon\n",
        "        \n",
        "    Returns:\n",
        "        features: (N, 1024) numpy array\n",
        "        labels: (N,) numpy array\n",
        "    \"\"\"\n",
        "    backbone.eval()\n",
        "    if generator is not None:\n",
        "        generator.eval()\n",
        "    \n",
        "    all_features = []\n",
        "    all_labels = []\n",
        "    count = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(loader, desc=\"Extracting features\"):\n",
        "            images = batch['img'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "            \n",
        "            if generator is not None:\n",
        "                # Generate adversarial images\n",
        "                # For targeted attack, use random targets\n",
        "                target_labels = torch.randint(0, NUM_CLASSES, labels.shape).to(device)\n",
        "                mask = (target_labels == labels)\n",
        "                target_labels[mask] = (target_labels[mask] + 1) % NUM_CLASSES\n",
        "                \n",
        "                # Check if generator expects target labels (ViT) or not (UNet)\n",
        "                try:\n",
        "                    noise = generator(images, target_labels)\n",
        "                except TypeError:\n",
        "                    # UNet doesn't take target labels\n",
        "                    noise = generator(images)\n",
        "                \n",
        "                noise = torch.clamp(noise, -eps, eps)\n",
        "                images = torch.clamp(images + noise, 0, 1)\n",
        "            \n",
        "            # Extract features\n",
        "            features = backbone(images)\n",
        "            \n",
        "            all_features.append(features.cpu().numpy())\n",
        "            all_labels.append(labels.cpu().numpy())\n",
        "            \n",
        "            count += len(labels)\n",
        "            if count >= num_samples:\n",
        "                break\n",
        "    \n",
        "    features = np.concatenate(all_features, axis=0)[:num_samples]\n",
        "    labels = np.concatenate(all_labels, axis=0)[:num_samples]\n",
        "    \n",
        "    return features, labels\n",
        "\n",
        "print(\"✓ Feature extraction functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: t-SNE Visualization Functions\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "def run_tsne(features_dict, labels, title=\"t-SNE Visualization\", save_path=None, perplexity=30):\n",
        "    \"\"\"\n",
        "    Run t-SNE on multiple feature sets and plot them together.\n",
        "    \n",
        "    Args:\n",
        "        features_dict: dict of {name: features_array}\n",
        "        labels: class labels (same for all)\n",
        "        title: Plot title\n",
        "        save_path: Optional path to save figure\n",
        "    \"\"\"\n",
        "    # Combine all features for joint t-SNE\n",
        "    all_features = []\n",
        "    all_names = []\n",
        "    all_indices = []\n",
        "    \n",
        "    start_idx = 0\n",
        "    for name, feats in features_dict.items():\n",
        "        all_features.append(feats)\n",
        "        all_names.extend([name] * len(feats))\n",
        "        all_indices.append((start_idx, start_idx + len(feats)))\n",
        "        start_idx += len(feats)\n",
        "    \n",
        "    combined = np.concatenate(all_features, axis=0)\n",
        "    \n",
        "    print(f\"Running t-SNE on {combined.shape[0]} samples...\")\n",
        "    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42, init='pca', n_iter=1000)\n",
        "    embeddings = tsne.fit_transform(combined)\n",
        "    \n",
        "    # Plot\n",
        "    n_variants = len(features_dict)\n",
        "    fig, axes = plt.subplots(1, n_variants + 1, figsize=(6 * (n_variants + 1), 5))\n",
        "    \n",
        "    # Color by class\n",
        "    cmap = plt.cm.get_cmap('tab20', NUM_CLASSES)\n",
        "    \n",
        "    # Plot each variant separately\n",
        "    for idx, (name, (start, end)) in enumerate(zip(features_dict.keys(), all_indices)):\n",
        "        ax = axes[idx]\n",
        "        emb = embeddings[start:end]\n",
        "        scatter = ax.scatter(emb[:, 0], emb[:, 1], c=labels, cmap=cmap, s=8, alpha=0.6)\n",
        "        ax.set_title(f\"{name}\\n(colored by class)\", fontsize=11)\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "    \n",
        "    # Plot all overlaid (last panel)\n",
        "    ax = axes[-1]\n",
        "    markers = ['o', 's', '^', 'D', 'v', 'P']  # Different markers for each variant\n",
        "    colors = plt.cm.Set1(np.linspace(0, 1, len(features_dict)))\n",
        "    \n",
        "    for idx, ((name, (start, end)), color, marker) in enumerate(zip(\n",
        "            zip(features_dict.keys(), all_indices), colors, markers)):\n",
        "        emb = embeddings[start:end]\n",
        "        ax.scatter(emb[:, 0], emb[:, 1], c=[color], s=12, alpha=0.5, marker=marker, label=name)\n",
        "    \n",
        "    ax.legend(loc='upper right', fontsize=8)\n",
        "    ax.set_title(\"All Variants Overlaid\\n(colored by variant)\", fontsize=11)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    \n",
        "    plt.suptitle(title, fontsize=14, y=1.02)\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "        print(f\"Saved to {save_path}\")\n",
        "    \n",
        "    plt.show()\n",
        "    \n",
        "    return embeddings\n",
        "\n",
        "print(\"✓ t-SNE visualization functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: UNet Variants (Existing Checkpoints)\n",
        "\n",
        "Compare clean features vs adversarial features from UNet Vanilla and UNet Contrastive.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: Load UNet Generators\n",
        "DRIVE_ROOT = \"/content/drive/MyDrive/grad/comp_vision/hanson_loss/oxford_pets\"\n",
        "\n",
        "# Exact filenames from Drive\n",
        "UNET_VANILLA_PATH = f\"{DRIVE_ROOT}/unet-vanilla-pets.pt\"\n",
        "UNET_CONTRASTIVE_PATH = f\"{DRIVE_ROOT}/unet-contrastive-pets.pt\"\n",
        "\n",
        "def load_unet(path, name):\n",
        "    if os.path.exists(path):\n",
        "        gen = UNet().to(device)\n",
        "        gen.load_state_dict(torch.load(path, map_location=device))\n",
        "        gen.eval()\n",
        "        print(f\"✓ Loaded {name} from {path}\")\n",
        "        return gen\n",
        "    else:\n",
        "        print(f\"✗ {name} not found at {path}\")\n",
        "        return None\n",
        "\n",
        "unet_vanilla = load_unet(UNET_VANILLA_PATH, \"UNet Vanilla\")\n",
        "unet_contrastive = load_unet(UNET_CONTRASTIVE_PATH, \"UNet Contrastive\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 8: Extract Features - UNet Variants\n",
        "NUM_SAMPLES = 1000  # Adjust based on memory/speed\n",
        "EPS = 16/255.\n",
        "\n",
        "print(\"Extracting clean features...\")\n",
        "feat_clean, labels = extract_features(\n",
        "    test_loader, surrogate.backbone, generator=None, \n",
        "    num_samples=NUM_SAMPLES, device=device\n",
        ")\n",
        "print(f\"  Clean features shape: {feat_clean.shape}\")\n",
        "\n",
        "features_unet = {\"Clean\": feat_clean}\n",
        "\n",
        "if unet_vanilla is not None:\n",
        "    print(\"\\nExtracting UNet Vanilla adversarial features...\")\n",
        "    feat_unet_vanilla, _ = extract_features(\n",
        "        test_loader, surrogate.backbone, generator=unet_vanilla,\n",
        "        num_samples=NUM_SAMPLES, eps=EPS, device=device\n",
        "    )\n",
        "    features_unet[\"UNet Vanilla\"] = feat_unet_vanilla\n",
        "    print(f\"  UNet Vanilla shape: {feat_unet_vanilla.shape}\")\n",
        "    del unet_vanilla  # Free memory\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "if unet_contrastive is not None:\n",
        "    print(\"\\nExtracting UNet Contrastive adversarial features...\")\n",
        "    feat_unet_contrastive, _ = extract_features(\n",
        "        test_loader, surrogate.backbone, generator=unet_contrastive,\n",
        "        num_samples=NUM_SAMPLES, eps=EPS, device=device\n",
        "    )\n",
        "    features_unet[\"UNet Contrastive\"] = feat_unet_contrastive\n",
        "    print(f\"  UNet Contrastive shape: {feat_unet_contrastive.shape}\")\n",
        "    del unet_contrastive\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(f\"\\n✓ Extracted features for: {list(features_unet.keys())}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 9: t-SNE Visualization - UNet Variants\n",
        "if len(features_unet) > 1:\n",
        "    run_tsne(\n",
        "        features_unet, \n",
        "        labels,\n",
        "        title=\"t-SNE: Clean vs UNet Adversarial Features (Oxford Pets)\",\n",
        "        save_path=\"/content/tsne_unet_comparison.png\"\n",
        "    )\n",
        "else:\n",
        "    print(\"Need at least 2 feature sets to compare. Check that UNet checkpoints loaded.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: ViT Variants (After Training)\n",
        "\n",
        "Run these cells after training the 3 ViT variants with `vit_train_colab.ipynb`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 10: Load ViT Generators (run after training completes)\n",
        "DRIVE_ROOT = \"/content/drive/MyDrive/grad/comp_vision/hanson_loss/oxford_pets\"\n",
        "\n",
        "vit_checkpoints = {\n",
        "    \"ViT Targeted\": f\"{DRIVE_ROOT}/vit_generator_targeted_only.pt\",\n",
        "    \"ViT Contrastive\": f\"{DRIVE_ROOT}/vit_generator_contrastive.pt\",\n",
        "    \"ViT Mixed\": f\"{DRIVE_ROOT}/vit_generator_mixed.pt\",\n",
        "}\n",
        "\n",
        "vit_generators = {}\n",
        "\n",
        "for name, path in vit_checkpoints.items():\n",
        "    if os.path.exists(path):\n",
        "        gen = ViTGenerator(num_classes=NUM_CLASSES).to(device)\n",
        "        gen.load_state_dict(torch.load(path, map_location=device), strict=False)\n",
        "        gen.eval()\n",
        "        vit_generators[name] = gen\n",
        "        print(f\"✓ Loaded {name} from {path}\")\n",
        "    else:\n",
        "        print(f\"✗ {name} not found at {path}\")\n",
        "\n",
        "print(f\"\\nLoaded {len(vit_generators)} ViT generators\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 11: Extract Features - ViT Variants\n",
        "NUM_SAMPLES = 1000\n",
        "EPS = 16/255.\n",
        "\n",
        "# Reuse clean features from earlier, or re-extract\n",
        "if 'feat_clean' not in dir() or feat_clean is None:\n",
        "    print(\"Extracting clean features...\")\n",
        "    feat_clean, labels = extract_features(\n",
        "        test_loader, surrogate.backbone, generator=None,\n",
        "        num_samples=NUM_SAMPLES, device=device\n",
        "    )\n",
        "\n",
        "features_vit = {\"Clean\": feat_clean}\n",
        "\n",
        "for name, gen in vit_generators.items():\n",
        "    print(f\"\\nExtracting {name} adversarial features...\")\n",
        "    feat_adv, _ = extract_features(\n",
        "        test_loader, surrogate.backbone, generator=gen,\n",
        "        num_samples=NUM_SAMPLES, eps=EPS, device=device\n",
        "    )\n",
        "    features_vit[name] = feat_adv\n",
        "    print(f\"  {name} shape: {feat_adv.shape}\")\n",
        "    \n",
        "    # Free memory after each\n",
        "    del gen\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Clear the dict since we deleted generators\n",
        "vit_generators.clear()\n",
        "\n",
        "print(f\"\\n✓ Extracted features for: {list(features_vit.keys())}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 12: t-SNE Visualization - ViT Variants\n",
        "if len(features_vit) > 1:\n",
        "    run_tsne(\n",
        "        features_vit,\n",
        "        labels,\n",
        "        title=\"t-SNE: Clean vs ViT Adversarial Features (Oxford Pets)\",\n",
        "        save_path=\"/content/tsne_vit_comparison.png\"\n",
        "    )\n",
        "else:\n",
        "    print(\"Need ViT checkpoints to run this. Train them first with vit_train_colab.ipynb\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Combined Comparison (All Generators)\n",
        "\n",
        "Compare all available generators together: UNet + ViT variants.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 13: Combined t-SNE - All Generators\n",
        "# Merge UNet and ViT features (excluding duplicated \"Clean\")\n",
        "features_all = {\"Clean\": feat_clean}\n",
        "\n",
        "# Add UNet features if available\n",
        "if 'features_unet' in dir():\n",
        "    for k, v in features_unet.items():\n",
        "        if k != \"Clean\":\n",
        "            features_all[k] = v\n",
        "\n",
        "# Add ViT features if available  \n",
        "if 'features_vit' in dir():\n",
        "    for k, v in features_vit.items():\n",
        "        if k != \"Clean\":\n",
        "            features_all[k] = v\n",
        "\n",
        "print(f\"Combined features: {list(features_all.keys())}\")\n",
        "\n",
        "if len(features_all) > 2:\n",
        "    run_tsne(\n",
        "        features_all,\n",
        "        labels,\n",
        "        title=\"t-SNE: All Generator Variants Compared (Oxford Pets)\",\n",
        "        save_path=\"/content/tsne_all_generators.png\"\n",
        "    )\n",
        "else:\n",
        "    print(\"Need more generator variants to make a meaningful comparison.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Quantitative Analysis\n",
        "\n",
        "Measure how much each generator disrupts the feature space.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 14: Quantitative Feature Disruption Analysis\n",
        "from scipy.spatial.distance import cosine\n",
        "import pandas as pd\n",
        "\n",
        "def compute_disruption_metrics(feat_clean, feat_adv):\n",
        "    \"\"\"\n",
        "    Compute metrics measuring how much adversarial features deviate from clean.\n",
        "    \"\"\"\n",
        "    # Per-sample cosine distance\n",
        "    cosine_dists = []\n",
        "    for i in range(len(feat_clean)):\n",
        "        cosine_dists.append(cosine(feat_clean[i], feat_adv[i]))\n",
        "    cosine_dists = np.array(cosine_dists)\n",
        "    \n",
        "    # L2 distance\n",
        "    l2_dists = np.linalg.norm(feat_clean - feat_adv, axis=1)\n",
        "    \n",
        "    return {\n",
        "        'cosine_dist_mean': np.mean(cosine_dists),\n",
        "        'cosine_dist_std': np.std(cosine_dists),\n",
        "        'l2_dist_mean': np.mean(l2_dists),\n",
        "        'l2_dist_std': np.std(l2_dists),\n",
        "    }\n",
        "\n",
        "# Compute for all variants\n",
        "results = []\n",
        "for name, feats in features_all.items():\n",
        "    if name == \"Clean\":\n",
        "        continue\n",
        "    metrics = compute_disruption_metrics(feat_clean, feats)\n",
        "    metrics['Generator'] = name\n",
        "    results.append(metrics)\n",
        "\n",
        "if results:\n",
        "    df = pd.DataFrame(results)\n",
        "    df = df[['Generator', 'cosine_dist_mean', 'cosine_dist_std', 'l2_dist_mean', 'l2_dist_std']]\n",
        "    df = df.sort_values('cosine_dist_mean', ascending=False)\n",
        "    \n",
        "    print(\"Feature Disruption Metrics (higher = more disruption):\")\n",
        "    print(\"=\" * 70)\n",
        "    print(df.to_string(index=False))\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "    # Plot\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "    \n",
        "    colors = plt.cm.Set2(np.linspace(0, 1, len(df)))\n",
        "    \n",
        "    ax = axes[0]\n",
        "    bars = ax.barh(df['Generator'], df['cosine_dist_mean'], xerr=df['cosine_dist_std'], color=colors)\n",
        "    ax.set_xlabel('Mean Cosine Distance')\n",
        "    ax.set_title('Feature Disruption (Cosine Distance)')\n",
        "    ax.invert_yaxis()\n",
        "    \n",
        "    ax = axes[1]\n",
        "    bars = ax.barh(df['Generator'], df['l2_dist_mean'], xerr=df['l2_dist_std'], color=colors)\n",
        "    ax.set_xlabel('Mean L2 Distance')\n",
        "    ax.set_title('Feature Disruption (L2 Distance)')\n",
        "    ax.invert_yaxis()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('/content/feature_disruption_metrics.png', dpi=150)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Need generator features to compute disruption metrics.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 15: Save All Results to Drive\n",
        "import shutil\n",
        "\n",
        "SAVE_DIR = \"/content/drive/MyDrive/grad/comp_vision/hanson_loss/oxford_pets/tsne_results\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "files_to_save = [\n",
        "    \"/content/tsne_unet_comparison.png\",\n",
        "    \"/content/tsne_vit_comparison.png\", \n",
        "    \"/content/tsne_all_generators.png\",\n",
        "    \"/content/feature_disruption_metrics.png\",\n",
        "]\n",
        "\n",
        "print(f\"Saving results to {SAVE_DIR}...\")\n",
        "for f in files_to_save:\n",
        "    if os.path.exists(f):\n",
        "        shutil.copy(f, SAVE_DIR)\n",
        "        print(f\"  ✓ {os.path.basename(f)}\")\n",
        "    else:\n",
        "        print(f\"  ✗ {os.path.basename(f)} (not found)\")\n",
        "\n",
        "print(\"\\nDone!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
